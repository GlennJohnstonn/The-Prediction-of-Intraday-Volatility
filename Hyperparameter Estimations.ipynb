{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9304bce-9c8a-419f-89bf-65bc81172136",
   "metadata": {},
   "source": [
    "This notebook is dedicated to the hyper parameter estimation of the Neural Networks and Gradient Boosting Decision Tree Algorithms.\n",
    "I use Otpuna for computational efficiency.This is more efficient than grid search algorithms as it uses performs Bayesian optimisation through the tree-structured parzen estimator, making it a realistic algorithm to preform, giving computational limitations. **Note:** due to limited computational power, I was unable to run an Optuna for CNN. Instead I used parameters from [Nyanpn's Kaggle Solution](https://www.kaggle.com/code/nyanpn/1st-place-public-2nd-place-solution?scriptVersionId=85907908)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac7d9a8-0fc2-4755-929c-cf2083d68eed",
   "metadata": {},
   "source": [
    "Firstly reusing the same settings as used in Nearest Neighbours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67042fab-2b23-4b51-9707-30cdc2d19f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "import ipywidgets as widgets\n",
    "import gc\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "import traceback\n",
    "from contextlib import contextmanager\n",
    "from enum import Enum\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from joblib import delayed, Parallel\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import statsmodels.api as sm\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    s = time.time()\n",
    "    yield\n",
    "    e = time.time() - s\n",
    "    print(f\"[{name}] {e:.3f}sec\")\n",
    "\n",
    "def print_trace(name: str = ''):\n",
    "    print(f'ERROR RAISED IN {name or \"anonymous\"}')\n",
    "    print(traceback.format_exc())\n",
    "\n",
    "DATA_DIR = 'data'\n",
    "# model & ensemble configurations\n",
    "PREDICT_CNN = True\n",
    "PREDICT_MLP = True\n",
    "PREDICT_GBDT = True\n",
    "PREDICT_TABNET = False\n",
    "\n",
    "GBDT_NUM_MODELS = 5 #3\n",
    "GBDT_LR = 0.02  # 0.1\n",
    "\n",
    "NN_VALID_TH = 0.25\n",
    "NN_MODEL_TOP_N = 3\n",
    "TAB_MODEL_TOP_N = 3\n",
    "ENSEMBLE_METHOD = 'mean'\n",
    "NN_NUM_MODELS = 10\n",
    "TABNET_NUM_MODELS = 5\n",
    "\n",
    "# data configurations\n",
    "USE_PRECOMPUTE_FEATURES = True  # Load precomputed features for train.csv from private dataset (just for speed up)\n",
    "\n",
    "\n",
    "# for saving quota\n",
    "IS_1ST_STAGE = True\n",
    "SHORTCUT_NN_IN_1ST_STAGE = False  # early-stop training to save GPU quota\n",
    "SHORTCUT_GBDT_IN_1ST_STAGE = False\n",
    "MEMORY_TEST_MODE = False\n",
    "\n",
    "# for ablation studies\n",
    "CV_SPLIT = 'time'  # 'time': time-series KFold 'group': GroupKFold by stock-id\n",
    "USE_PRICE_NN_FEATURES = True  # Use nearest neighbor features that rely on tick size\n",
    "USE_VOL_NN_FEATURES = True  # Use nearest neighbor features that can be calculated without tick size\n",
    "USE_SIZE_NN_FEATURES = True  # Use nearest neighbor features that can be calculated without tick size\n",
    "USE_RANDOM_NN_FEATURES = False  # Use random index to aggregate neighbors\n",
    "\n",
    "USE_TIME_ID_NN = True  # Use time-id based neighbors\n",
    "USE_STOCK_ID_NN = True  # Use stock-id based neighbors\n",
    "\n",
    "ENABLE_RANK_NORMALIZATION = True  # Enable rank-normalization\n",
    "\n",
    "train = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'))\n",
    "stock_ids = set(train['stock_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9534ca4c-d9c0-4ca7-8bc8-50d55fb910b7",
   "metadata": {},
   "source": [
    "Loading Training Dataset after performing Nearest Neighbours Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3001893b-e524-4cde-9d5c-288c269b6fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X(df_src):\n",
    "    cols = [c for c in df_src.columns if c not in ['time_id', 'target', 'tick_size']]\n",
    "    return df_src[cols]\n",
    "    \n",
    "df_train = pd.read_csv(\"df_train.csv\")\n",
    "df_test = pd.read_csv(\"df_test.csv\")\n",
    "X = get_X(df_train)            #remove non-feature columns (time_id, target, tick_size).\n",
    "y = 2*np.log(df_train['target'] +1e-9) #will train models and cross validate models using log-rv as this allows for better optimisation \n",
    "                                        #and allows me to compute error metrics backed by academics    \n",
    "\n",
    "del df_train\n",
    "folds = np.load(\"folds.npy\", allow_pickle=True)\n",
    "\n",
    "# Load from Pickle (.pkl)\n",
    "with open(\"folds.pkl\", \"rb\") as f:\n",
    "    folds = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca05cd0-87ef-4595-93b0-cdc3bedc3cbf",
   "metadata": {},
   "source": [
    "A couple more definitions to ensure code runs efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5facd99-4095-4a3b-91b1-7c2ff41a5570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmspe(y_true, y_pred):\n",
    "    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n",
    "\n",
    "\n",
    "def feval_RMSPE(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    return 'RMSPE', round(rmspe(y_true = labels, y_pred = preds),5), False\n",
    "\n",
    "\n",
    "# from: https://blog.amedama.jp/entry/lightgbm-cv-feature-importance\n",
    "def plot_importance(cvbooster, figsize=(10, 10)):\n",
    "    raw_importances = cvbooster.feature_importance(importance_type='gain')\n",
    "    feature_name = cvbooster.boosters[0].feature_name()\n",
    "    importance_df = pd.DataFrame(data=raw_importances,\n",
    "                                 columns=feature_name)\n",
    "    # order by average importance across folds\n",
    "    sorted_indices = importance_df.mean(axis=0).sort_values(ascending=False).index\n",
    "    sorted_importance_df = importance_df.loc[:, sorted_indices]\n",
    "    # plot top-n\n",
    "    PLOT_TOP_N = 50\n",
    "    plot_cols = sorted_importance_df.columns[:PLOT_TOP_N]\n",
    "    _, ax = plt.subplots(figsize=figsize)\n",
    "    ax.grid()\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_ylabel('Feature')\n",
    "    ax.set_xlabel('Importance')\n",
    "    sns.boxplot(data=sorted_importance_df[plot_cols],\n",
    "                orient='h',\n",
    "                ax=ax)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_X(df_src):\n",
    "    cols = [c for c in df_src.columns if c not in ['time_id', 'target', 'tick_size']]\n",
    "    return df_src[cols]\n",
    "\n",
    "\n",
    "class EnsembleModel:\n",
    "    def __init__(self, models: List[lgb.Booster], weights: Optional[List[float]] = None):\n",
    "        self.models = models\n",
    "        self.weights = weights\n",
    "\n",
    "        features = list(self.models[0].feature_name())\n",
    "\n",
    "        for m in self.models[1:]:\n",
    "            assert features == list(m.feature_name())\n",
    "#Checks that all models use the same features.\n",
    "    def predict(self, x):\n",
    "        predicted = np.zeros((len(x), len(self.models)))\n",
    "\n",
    "        for i, m in enumerate(self.models):\n",
    "            w = self.weights[i] if self.weights is not None else 1\n",
    "            predicted[:, i] = w * m.predict(x)\n",
    "\n",
    "        ttl = np.sum(self.weights) if self.weights is not None else len(self.models)\n",
    "        return np.sum(predicted, axis=1) / ttl\n",
    "\n",
    "    #Runs predictions for each model and stores them in predicted[:, i].\n",
    "    #If weights are given, normalizes predictions using np.sum(self.weights). Otherwise, averages predictions across models.\n",
    "    \n",
    "    def feature_name(self) -> List[str]:\n",
    "        return self.models[0].feature_name()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2407309a-96ac-460a-b211-e8e2291e7aac",
   "metadata": {},
   "source": [
    "Installing Optuna Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b67ba86-9e0a-42d5-988d-bf9a7577fa01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\programdata\\anaconda3\\lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (1.11.4)\n",
      "Requirement already satisfied: optuna-integration[xgboost] in c:\\programdata\\anaconda3\\lib\\site-packages (4.2.1)\n",
      "Requirement already satisfied: optuna in c:\\programdata\\anaconda3\\lib\\site-packages (from optuna-integration[xgboost]) (4.2.1)\n",
      "Requirement already satisfied: xgboost in c:\\programdata\\anaconda3\\lib\\site-packages (from optuna-integration[xgboost]) (3.0.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from optuna->optuna-integration[xgboost]) (1.15.2)\n",
      "Requirement already satisfied: colorlog in c:\\programdata\\anaconda3\\lib\\site-packages (from optuna->optuna-integration[xgboost]) (6.9.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from optuna->optuna-integration[xgboost]) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from optuna->optuna-integration[xgboost]) (23.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from optuna->optuna-integration[xgboost]) (2.0.25)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from optuna->optuna-integration[xgboost]) (4.65.0)\n",
      "Requirement already satisfied: PyYAML in c:\\programdata\\anaconda3\\lib\\site-packages (from optuna->optuna-integration[xgboost]) (6.0.1)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost->optuna-integration[xgboost]) (1.11.4)\n",
      "Requirement already satisfied: Mako in c:\\programdata\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna->optuna-integration[xgboost]) (1.3.9)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in c:\\programdata\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna->optuna-integration[xgboost]) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna->optuna-integration[xgboost]) (3.0.1)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from colorlog->optuna->optuna-integration[xgboost]) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna->optuna-integration[xgboost]) (2.1.3)\n",
      "Requirement already satisfied: optuna in c:\\programdata\\anaconda3\\lib\\site-packages (4.2.1)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from optuna) (1.15.2)\n",
      "Requirement already satisfied: colorlog in c:\\programdata\\anaconda3\\lib\\site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from optuna) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from optuna) (23.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from optuna) (2.0.25)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from optuna) (4.65.0)\n",
      "Requirement already satisfied: PyYAML in c:\\programdata\\anaconda3\\lib\\site-packages (from optuna) (6.0.1)\n",
      "Requirement already satisfied: Mako in c:\\programdata\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.3.9)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in c:\\programdata\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna) (3.0.1)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
      "Requirement already satisfied: lightgbm in c:\\programdata\\anaconda3\\lib\\site-packages (4.6.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from lightgbm) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from lightgbm) (1.11.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade xgboost\n",
    "!pip install optuna-integration[xgboost]\n",
    "!pip install optuna\n",
    "!pip install lightgbm\n",
    "import lightgbm as lgb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e59450-e6f3-4115-b2f6-2a0d1cf8df33",
   "metadata": {},
   "source": [
    "# XGBoost Optuna Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e305d565-3bc0-498f-8e11-68cfde777ddc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-28 18:22:44,316] A new study created in memory with name: no-name-16b9d046-d459-4c1e-abbe-7c39abb01e75\n",
      "[I 2025-03-28 18:38:51,723] Trial 0 finished with value: 0.08236368103678064 and parameters: {'learning_rate': 0.033195054155555, 'max_depth': 9, 'n_estimators': 501, 'subsample': 0.7298267937721498, 'colsample_bytree': 0.7982340514709187, 'lambda': 0.0012848548354308237, 'alpha': 0.06346749040163628}. Best is trial 0 with value: 0.08236368103678064.\n",
      "[I 2025-03-28 18:51:11,647] Trial 1 finished with value: 0.08723894339464308 and parameters: {'learning_rate': 0.0540179714516467, 'max_depth': 3, 'n_estimators': 976, 'subsample': 0.6654845066640638, 'colsample_bytree': 0.8581888676761157, 'lambda': 0.6721732709454367, 'alpha': 0.0016163265824155825}. Best is trial 0 with value: 0.08236368103678064.\n",
      "[I 2025-03-28 18:55:26,106] Trial 2 finished with value: 0.0933578547476957 and parameters: {'learning_rate': 0.02446264104484393, 'max_depth': 8, 'n_estimators': 112, 'subsample': 0.7414689364796533, 'colsample_bytree': 0.5234802855076269, 'lambda': 0.10035173666328619, 'alpha': 0.01357224593416554}. Best is trial 0 with value: 0.08236368103678064.\n",
      "[I 2025-03-28 19:05:35,201] Trial 3 finished with value: 0.08433768084512698 and parameters: {'learning_rate': 0.0539023415264783, 'max_depth': 5, 'n_estimators': 676, 'subsample': 0.7116869497204911, 'colsample_bytree': 0.8427365303662537, 'lambda': 0.6094400824346802, 'alpha': 0.08430251143257116}. Best is trial 0 with value: 0.08236368103678064.\n",
      "[I 2025-03-28 19:28:05,490] Trial 4 finished with value: 0.0829141971816734 and parameters: {'learning_rate': 0.014740114350253908, 'max_depth': 9, 'n_estimators': 614, 'subsample': 0.8256404467469955, 'colsample_bytree': 0.7157937700594912, 'lambda': 0.422976905488224, 'alpha': 0.0012791548314455942}. Best is trial 0 with value: 0.08236368103678064.\n",
      "[I 2025-03-28 19:32:16,016] Trial 5 finished with value: 0.08983404775574994 and parameters: {'learning_rate': 0.21737125007267968, 'max_depth': 10, 'n_estimators': 837, 'subsample': 0.6914194134963361, 'colsample_bytree': 0.7501780975923023, 'lambda': 0.0028272460352991757, 'alpha': 0.010222702164443775}. Best is trial 0 with value: 0.08236368103678064.\n",
      "[I 2025-03-28 19:39:16,236] Trial 6 finished with value: 0.0883668051644946 and parameters: {'learning_rate': 0.019725210779011383, 'max_depth': 8, 'n_estimators': 191, 'subsample': 0.6985979029509728, 'colsample_bytree': 0.7723798703014564, 'lambda': 0.11450217465566653, 'alpha': 0.07618693003520066}. Best is trial 0 with value: 0.08236368103678064.\n",
      "[I 2025-03-28 19:45:17,494] Trial 7 finished with value: 0.08399986071376657 and parameters: {'learning_rate': 0.12557145003511513, 'max_depth': 8, 'n_estimators': 752, 'subsample': 0.8259855019439581, 'colsample_bytree': 0.7360891830259506, 'lambda': 0.04391759447501439, 'alpha': 0.005500648319533676}. Best is trial 0 with value: 0.08236368103678064.\n",
      "[I 2025-03-28 19:49:01,001] Trial 8 finished with value: 0.08566708252881994 and parameters: {'learning_rate': 0.2417779635448789, 'max_depth': 7, 'n_estimators': 257, 'subsample': 0.6759523020653369, 'colsample_bytree': 0.7546245548032394, 'lambda': 2.061181019905031, 'alpha': 2.5557610759418874}. Best is trial 0 with value: 0.08236368103678064.\n",
      "[I 2025-03-28 19:54:11,797] Trial 9 finished with value: 0.0844221872900242 and parameters: {'learning_rate': 0.07437502895175649, 'max_depth': 7, 'n_estimators': 196, 'subsample': 0.9699832356629208, 'colsample_bytree': 0.6146767762783378, 'lambda': 0.22522509274360802, 'alpha': 0.0038762160333777884}. Best is trial 0 with value: 0.08236368103678064.\n",
      "[I 2025-03-28 19:59:41,946] Trial 10 finished with value: 0.08569096706086593 and parameters: {'learning_rate': 0.17290490683495274, 'max_depth': 5, 'n_estimators': 478, 'subsample': 0.5549653006141275, 'colsample_bytree': 0.9466734859572674, 'lambda': 0.0010206787040385901, 'alpha': 1.5220862893130618}. Best is trial 0 with value: 0.08236368103678064.\n",
      "[I 2025-03-28 20:10:48,608] Trial 11 finished with value: 0.08316622678356086 and parameters: {'learning_rate': 0.10474894103488383, 'max_depth': 10, 'n_estimators': 475, 'subsample': 0.8729531456093425, 'colsample_bytree': 0.6458186441937961, 'lambda': 0.014516411265607427, 'alpha': 0.42873907498718167}. Best is trial 0 with value: 0.08236368103678064.\n",
      "[I 2025-03-28 20:32:46,947] Trial 12 finished with value: 0.08517902266862985 and parameters: {'learning_rate': 0.010925458956627868, 'max_depth': 9, 'n_estimators': 575, 'subsample': 0.8420558616841491, 'colsample_bytree': 0.656022277016993, 'lambda': 5.84716946504812, 'alpha': 9.48235077176785}. Best is trial 0 with value: 0.08236368103678064.\n",
      "[I 2025-03-28 20:37:45,880] Trial 13 finished with value: 0.0870663528765194 and parameters: {'learning_rate': 0.1644300789863664, 'max_depth': 9, 'n_estimators': 351, 'subsample': 0.578073744832563, 'colsample_bytree': 0.8505375176132329, 'lambda': 0.013205222876060008, 'alpha': 0.001029076293195403}. Best is trial 0 with value: 0.08236368103678064.\n",
      "[I 2025-03-28 20:44:41,411] Trial 14 finished with value: 0.08437004067126333 and parameters: {'learning_rate': 0.10860303713655646, 'max_depth': 6, 'n_estimators': 612, 'subsample': 0.7974716496052282, 'colsample_bytree': 0.979560314239295, 'lambda': 0.017679659167782653, 'alpha': 0.03553294834933105}. Best is trial 0 with value: 0.08236368103678064.\n",
      "[I 2025-03-28 20:55:27,330] Trial 15 finished with value: 0.08241026188076064 and parameters: {'learning_rate': 0.07907030278753333, 'max_depth': 9, 'n_estimators': 384, 'subsample': 0.9273503156284295, 'colsample_bytree': 0.6865777983294112, 'lambda': 0.0026526338295507295, 'alpha': 0.5439435483039575}. Best is trial 0 with value: 0.08236368103678064.\n",
      "[I 2025-03-28 20:59:06,575] Trial 16 finished with value: 0.08914394504982201 and parameters: {'learning_rate': 0.2878886869602969, 'max_depth': 10, 'n_estimators': 367, 'subsample': 0.9903737032679507, 'colsample_bytree': 0.555416272816526, 'lambda': 0.0025447297314975838, 'alpha': 0.3991686919756727}. Best is trial 0 with value: 0.08236368103678064.\n",
      "[I 2025-03-28 21:05:05,913] Trial 17 finished with value: 0.08805448507851889 and parameters: {'learning_rate': 0.08223750533568866, 'max_depth': 3, 'n_estimators': 418, 'subsample': 0.9186550771918333, 'colsample_bytree': 0.8956170721146401, 'lambda': 0.004580001961833747, 'alpha': 0.3416934814520511}. Best is trial 0 with value: 0.08236368103678064.\n",
      "[I 2025-03-28 21:09:35,233] Trial 18 finished with value: 0.08551627961928511 and parameters: {'learning_rate': 0.13614259200943565, 'max_depth': 6, 'n_estimators': 316, 'subsample': 0.6132304256510431, 'colsample_bytree': 0.5890749737568354, 'lambda': 0.0011161330446957743, 'alpha': 0.19036604162549642}. Best is trial 0 with value: 0.08236368103678064.\n",
      "[I 2025-03-28 21:24:50,482] Trial 19 finished with value: 0.08124880641344534 and parameters: {'learning_rate': 0.053125796299066746, 'max_depth': 9, 'n_estimators': 465, 'subsample': 0.7795469313455295, 'colsample_bytree': 0.6821554767850635, 'lambda': 0.005897765426713198, 'alpha': 1.5735067529347877}. Best is trial 19 with value: 0.08124880641344534.\n",
      "[I 2025-03-28 21:36:31,801] Trial 20 finished with value: 0.08151735279107875 and parameters: {'learning_rate': 0.053547106241522216, 'max_depth': 8, 'n_estimators': 491, 'subsample': 0.7729887460865675, 'colsample_bytree': 0.7914850295090989, 'lambda': 0.00485719892052795, 'alpha': 1.726762246180695}. Best is trial 19 with value: 0.08124880641344534.\n",
      "[I 2025-03-28 21:50:39,239] Trial 21 finished with value: 0.0814654771126074 and parameters: {'learning_rate': 0.04530973388303504, 'max_depth': 8, 'n_estimators': 516, 'subsample': 0.7635211875639552, 'colsample_bytree': 0.7999342616757674, 'lambda': 0.009382353603569232, 'alpha': 1.6811911501161703}. Best is trial 19 with value: 0.08124880641344534.\n",
      "[I 2025-03-28 22:07:06,986] Trial 22 finished with value: 0.08102914883038144 and parameters: {'learning_rate': 0.05546069868577161, 'max_depth': 8, 'n_estimators': 693, 'subsample': 0.7826599361591621, 'colsample_bytree': 0.7948978100031234, 'lambda': 0.008850044286023487, 'alpha': 2.099910834121656}. Best is trial 22 with value: 0.08102914883038144.\n",
      "[I 2025-03-28 22:21:46,315] Trial 23 finished with value: 0.08128174710140657 and parameters: {'learning_rate': 0.09232551016209636, 'max_depth': 7, 'n_estimators': 719, 'subsample': 0.7768896192237988, 'colsample_bytree': 0.6969558518090859, 'lambda': 0.03649119722903818, 'alpha': 4.968991233596287}. Best is trial 22 with value: 0.08102914883038144.\n",
      "[I 2025-03-28 22:38:30,684] Trial 24 finished with value: 0.08045193380203047 and parameters: {'learning_rate': 0.09971287766973493, 'max_depth': 7, 'n_estimators': 801, 'subsample': 0.8763360101347328, 'colsample_bytree': 0.698114590592158, 'lambda': 0.042531472780104954, 'alpha': 6.997454407950201}. Best is trial 24 with value: 0.08045193380203047.\n",
      "[I 2025-03-28 22:52:27,367] Trial 25 finished with value: 0.08099104313882972 and parameters: {'learning_rate': 0.13807477801047874, 'max_depth': 6, 'n_estimators': 859, 'subsample': 0.899915493173033, 'colsample_bytree': 0.6541068956318704, 'lambda': 0.03758876652144196, 'alpha': 9.799584517506721}. Best is trial 24 with value: 0.08045193380203047.\n",
      "[I 2025-03-28 23:03:36,101] Trial 26 finished with value: 0.08250455525559705 and parameters: {'learning_rate': 0.13935037433863293, 'max_depth': 5, 'n_estimators': 875, 'subsample': 0.8890714180948415, 'colsample_bytree': 0.5973051370801239, 'lambda': 0.036877432451030645, 'alpha': 8.05380608399926}. Best is trial 24 with value: 0.08045193380203047.\n",
      "[I 2025-03-28 23:11:10,867] Trial 27 finished with value: 0.08330956729677017 and parameters: {'learning_rate': 0.1873745006903009, 'max_depth': 6, 'n_estimators': 816, 'subsample': 0.9358093923950229, 'colsample_bytree': 0.6415554037870494, 'lambda': 0.058383084744617356, 'alpha': 3.4740882181573385}. Best is trial 24 with value: 0.08045193380203047.\n",
      "[I 2025-03-28 23:24:49,911] Trial 28 finished with value: 0.08307077052200354 and parameters: {'learning_rate': 0.11507240421760452, 'max_depth': 4, 'n_estimators': 973, 'subsample': 0.8686112120472829, 'colsample_bytree': 0.9035312570851188, 'lambda': 0.022902655156055125, 'alpha': 5.389145597272885}. Best is trial 24 with value: 0.08045193380203047.\n",
      "[I 2025-03-28 23:32:52,359] Trial 29 finished with value: 0.083078797448456 and parameters: {'learning_rate': 0.15166599043609053, 'max_depth': 7, 'n_estimators': 894, 'subsample': 0.9036097756631513, 'colsample_bytree': 0.813906143381407, 'lambda': 0.17518196744873799, 'alpha': 1.0074454286892753}. Best is trial 24 with value: 0.08045193380203047.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Best Hyperparameters (Q-Likelihood Optimized): {'learning_rate': 0.09971287766973493, 'max_depth': 7, 'n_estimators': 801, 'subsample': 0.8763360101347328, 'colsample_bytree': 0.698114590592158, 'lambda': 0.042531472780104954, 'alpha': 6.997454407950201}\n",
      "\n",
      "✅ XGBoost training completed (QLIKE minimized)!\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load predefined folds\n",
    "with open(\"folds.pkl\", \"rb\") as f:\n",
    "    folds = pickle.load(f)\n",
    "\n",
    "# Define Q-Likelihood metric\n",
    "def qlike_from_rv(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # Avoid divide-by-zero or log(0)\n",
    "    eps = 1e-9\n",
    "\n",
    "    exp_y_true = np.exp(y_true) + eps\n",
    "    exp_y_pred = np.exp(y_pred) + eps\n",
    "\n",
    "    qlike = (exp_y_true / exp_y_pred) - (y_true - y_pred) - 1\n",
    "    return np.mean(qlike)\n",
    "\n",
    "# Define Optuna objective\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',  # still needed for early stopping\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'lambda': trial.suggest_float('lambda', 1e-3, 10.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-3, 10.0, log=True),\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 42,\n",
    "        'early_stopping_rounds': 50\n",
    "    }\n",
    "\n",
    "    qlike_scores = []\n",
    "    for fold_idx, (train_idx, valid_idx) in enumerate(folds):\n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "\n",
    "        model = xgb.XGBRegressor(**params)\n",
    "\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_valid, y_valid)],\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        y_pred = model.predict(X_valid)\n",
    "        qlike_score = qlike_from_rv(y_valid, y_pred)\n",
    "        qlike_scores.append(qlike_score)\n",
    "\n",
    "    return np.mean(qlike_scores)  # Optuna will minimize this\n",
    "\n",
    "# Run Optuna optimization\n",
    "study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler())\n",
    "study.optimize(objective, n_trials=30, timeout=3600*10)  # 30 trials, max 10 hours\n",
    "\n",
    "# Best parameters\n",
    "best_params = study.best_params\n",
    "print(\"\\n🔹 Best Hyperparameters (Q-Likelihood Optimized):\", best_params)\n",
    "\n",
    "# Train final model with best parameters on full data\n",
    "final_model = xgb.XGBRegressor(**best_params, random_state=42)\n",
    "final_model.fit(X, y)\n",
    "\n",
    "# Save final model\n",
    "final_model.save_model(\"xgboost_optuna_qlike.json\")\n",
    "\n",
    "print(\"\\n✅ XGBoost training completed (QLIKE minimized)!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fd7dd7-1291-41dc-9393-2d4a093f22ca",
   "metadata": {},
   "source": [
    "# LightGBM Optuna run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d922956a-cd0c-4aa6-9e73-61d880ca060e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-28 23:37:03,997] A new study created in memory with name: LightGBM_QLIKE_GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Starting Optuna hyperparameter tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-28 23:40:30,283] Trial 0 finished with value: 0.08633851203061542 and parameters: {'learning_rate': 0.15078934712281716, 'num_leaves': 254, 'min_data_in_leaf': 373, 'reg_alpha': 2.931123275823684, 'reg_lambda': 2.6441165567498457, 'colsample_bytree': 0.3395837701497705}. Best is trial 0 with value: 0.08633851203061542.\n",
      "[I 2025-03-29 00:04:29,558] Trial 1 finished with value: 0.09003240930857781 and parameters: {'learning_rate': 0.011830027015974607, 'num_leaves': 274, 'min_data_in_leaf': 335, 'reg_alpha': 8.258269636322225, 'reg_lambda': 9.731154574846657, 'colsample_bytree': 0.48429967603906177}. Best is trial 0 with value: 0.08633851203061542.\n",
      "[I 2025-03-29 00:10:31,586] Trial 2 finished with value: 0.08693826578184258 and parameters: {'learning_rate': 0.07123504214479567, 'num_leaves': 324, 'min_data_in_leaf': 1725, 'reg_alpha': 3.880713991101804, 'reg_lambda': 1.7068787655623392, 'colsample_bytree': 0.7690905870548737}. Best is trial 0 with value: 0.08633851203061542.\n",
      "[I 2025-03-29 00:56:38,981] Trial 3 finished with value: 0.08720611471268025 and parameters: {'learning_rate': 0.006216910696018457, 'num_leaves': 38, 'min_data_in_leaf': 757, 'reg_alpha': 4.874137871053926, 'reg_lambda': 8.13123352030619, 'colsample_bytree': 0.5845442661978841}. Best is trial 0 with value: 0.08633851203061542.\n",
      "[I 2025-03-29 01:16:36,259] Trial 4 finished with value: 0.09216627462201805 and parameters: {'learning_rate': 0.02921323414992162, 'num_leaves': 199, 'min_data_in_leaf': 1023, 'reg_alpha': 9.846763404883133, 'reg_lambda': 4.918381655560469, 'colsample_bytree': 0.10467683700322859}. Best is trial 0 with value: 0.08633851203061542.\n",
      "[I 2025-03-29 01:50:24,551] Trial 5 finished with value: 0.09043004523256895 and parameters: {'learning_rate': 0.006184506690250785, 'num_leaves': 374, 'min_data_in_leaf': 547, 'reg_alpha': 8.55609410084912, 'reg_lambda': 3.1737136984348426, 'colsample_bytree': 0.28384442855458336}. Best is trial 0 with value: 0.08633851203061542.\n",
      "[I 2025-03-29 02:52:14,471] Trial 6 finished with value: 0.0818868423560378 and parameters: {'learning_rate': 0.010999385607286381, 'num_leaves': 62, 'min_data_in_leaf': 190, 'reg_alpha': 1.1789382151470003, 'reg_lambda': 8.258210201230492, 'colsample_bytree': 0.3871060132973013}. Best is trial 6 with value: 0.0818868423560378.\n",
      "[I 2025-03-29 03:36:46,488] Trial 7 finished with value: 0.0852644158122687 and parameters: {'learning_rate': 0.01054501744515031, 'num_leaves': 319, 'min_data_in_leaf': 1432, 'reg_alpha': 2.9263385281672565, 'reg_lambda': 7.276658074258815, 'colsample_bytree': 0.9384751685724763}. Best is trial 6 with value: 0.0818868423560378.\n",
      "[I 2025-03-29 03:44:07,994] Trial 8 finished with value: 0.09123950615389881 and parameters: {'learning_rate': 0.05121148129894198, 'num_leaves': 367, 'min_data_in_leaf': 1940, 'reg_alpha': 9.021113258904176, 'reg_lambda': 7.182133188258454, 'colsample_bytree': 0.4537865804260204}. Best is trial 6 with value: 0.0818868423560378.\n",
      "[I 2025-03-29 03:47:25,589] Trial 9 finished with value: 0.09049121716733471 and parameters: {'learning_rate': 0.08880094723241136, 'num_leaves': 95, 'min_data_in_leaf': 1524, 'reg_alpha': 7.750792172478712, 'reg_lambda': 9.214796926192728, 'colsample_bytree': 0.6983296046549203}. Best is trial 6 with value: 0.0818868423560378.\n",
      "[I 2025-03-29 05:17:17,326] Trial 10 finished with value: 0.07900390325243073 and parameters: {'learning_rate': 0.02094170509434146, 'num_leaves': 461, 'min_data_in_leaf': 184, 'reg_alpha': 0.5763273726632206, 'reg_lambda': 0.2280411449010753, 'colsample_bytree': 0.11557603405530914}. Best is trial 10 with value: 0.07900390325243073.\n",
      "[I 2025-03-29 07:20:21,562] Trial 11 finished with value: 0.07761080182524772 and parameters: {'learning_rate': 0.02239772253014732, 'num_leaves': 506, 'min_data_in_leaf': 244, 'reg_alpha': 0.12281018980456526, 'reg_lambda': 0.17998083120283326, 'colsample_bytree': 0.10882799233263057}. Best is trial 11 with value: 0.07761080182524772.\n",
      "[I 2025-03-29 09:05:27,192] Trial 12 finished with value: 0.07768941445118871 and parameters: {'learning_rate': 0.023508233737021093, 'num_leaves': 486, 'min_data_in_leaf': 747, 'reg_alpha': 0.2067605395259142, 'reg_lambda': 0.17238254543728998, 'colsample_bytree': 0.11132894279386174}. Best is trial 11 with value: 0.07761080182524772.\n",
      "[I 2025-03-29 10:48:34,079] Trial 13 finished with value: 0.07739272995457555 and parameters: {'learning_rate': 0.021037853021298323, 'num_leaves': 511, 'min_data_in_leaf': 840, 'reg_alpha': 0.15479754480332078, 'reg_lambda': 0.19138023942119628, 'colsample_bytree': 0.23263032112768528}. Best is trial 13 with value: 0.07739272995457555.\n",
      "[I 2025-03-29 11:23:46,168] Trial 14 finished with value: 0.08421495450116392 and parameters: {'learning_rate': 0.017322933089645533, 'num_leaves': 441, 'min_data_in_leaf': 1186, 'reg_alpha': 2.0471629318880527, 'reg_lambda': 1.598364626219052, 'colsample_bytree': 0.2571213833972669}. Best is trial 13 with value: 0.07739272995457555.\n",
      "[I 2025-03-29 11:37:35,090] Trial 15 finished with value: 0.08919102967952927 and parameters: {'learning_rate': 0.04212310354938358, 'num_leaves': 494, 'min_data_in_leaf': 886, 'reg_alpha': 6.643459881417223, 'reg_lambda': 4.1877430236748525, 'colsample_bytree': 0.21234102669662427}. Best is trial 13 with value: 0.07739272995457555.\n",
      "[I 2025-03-29 11:54:30,408] Trial 16 finished with value: 0.08349637343984491 and parameters: {'learning_rate': 0.038509806847322546, 'num_leaves': 412, 'min_data_in_leaf': 550, 'reg_alpha': 1.718698357045623, 'reg_lambda': 1.304593275943158, 'colsample_bytree': 0.20960730745800896}. Best is trial 13 with value: 0.07739272995457555.\n",
      "[I 2025-03-29 12:21:20,381] Trial 17 finished with value: 0.08830771825607121 and parameters: {'learning_rate': 0.01304923661779668, 'num_leaves': 164, 'min_data_in_leaf': 1175, 'reg_alpha': 5.776114617310471, 'reg_lambda': 3.289114771741868, 'colsample_bytree': 0.36170849236347385}. Best is trial 13 with value: 0.07739272995457555.\n",
      "[I 2025-03-29 13:56:29,359] Trial 18 finished with value: 0.07731068281046054 and parameters: {'learning_rate': 0.028795081839245874, 'num_leaves': 512, 'min_data_in_leaf': 560, 'reg_alpha': 0.1829700145871248, 'reg_lambda': 5.773687488019248, 'colsample_bytree': 0.562008010008035}. Best is trial 18 with value: 0.07731068281046054.\n",
      "[I 2025-03-29 14:05:17,340] Trial 19 finished with value: 0.0850354654001048 and parameters: {'learning_rate': 0.058512495415056584, 'num_leaves': 414, 'min_data_in_leaf': 516, 'reg_alpha': 2.7152862056825313, 'reg_lambda': 6.235550148758833, 'colsample_bytree': 0.6657381821664441}. Best is trial 18 with value: 0.07731068281046054.\n",
      "[I 2025-03-29 14:09:27,418] Trial 20 finished with value: 0.08725466842023794 and parameters: {'learning_rate': 0.11509353226615147, 'num_leaves': 511, 'min_data_in_leaf': 755, 'reg_alpha': 4.257058592533652, 'reg_lambda': 6.076436202597064, 'colsample_bytree': 0.8763520387295953}. Best is trial 18 with value: 0.07731068281046054.\n",
      "[I 2025-03-29 16:03:25,176] Trial 21 finished with value: 0.07719162594810326 and parameters: {'learning_rate': 0.0278011551939974, 'num_leaves': 446, 'min_data_in_leaf': 423, 'reg_alpha': 0.1594560214369773, 'reg_lambda': 4.581336080011035, 'colsample_bytree': 0.5660088047082317}. Best is trial 21 with value: 0.07719162594810326.\n",
      "[I 2025-03-29 16:31:09,631] Trial 22 finished with value: 0.081542747798934 and parameters: {'learning_rate': 0.03399095303142508, 'num_leaves': 445, 'min_data_in_leaf': 452, 'reg_alpha': 1.1473735126697275, 'reg_lambda': 5.396944358048797, 'colsample_bytree': 0.5741617854865357}. Best is trial 21 with value: 0.07719162594810326.\n",
      "[I 2025-03-29 17:21:32,589] Trial 23 finished with value: 0.08202607739753082 and parameters: {'learning_rate': 0.016648544644731627, 'num_leaves': 418, 'min_data_in_leaf': 635, 'reg_alpha': 1.270555099312117, 'reg_lambda': 4.135111292999173, 'colsample_bytree': 0.5207361663323113}. Best is trial 21 with value: 0.07719162594810326.\n",
      "[I 2025-03-29 17:39:55,606] Trial 24 finished with value: 0.08455661420025923 and parameters: {'learning_rate': 0.02978362067675545, 'num_leaves': 465, 'min_data_in_leaf': 942, 'reg_alpha': 2.447683348496097, 'reg_lambda': 4.849618545881216, 'colsample_bytree': 0.6215143410626874}. Best is trial 21 with value: 0.07719162594810326.\n",
      "[I 2025-03-29 19:04:50,798] Trial 25 finished with value: 0.07946628283567578 and parameters: {'learning_rate': 0.01611669762085087, 'num_leaves': 375, 'min_data_in_leaf': 673, 'reg_alpha': 0.7151122299838386, 'reg_lambda': 6.338268805838389, 'colsample_bytree': 0.7757620318713212}. Best is trial 21 with value: 0.07719162594810326.\n",
      "[I 2025-03-29 19:51:53,745] Trial 26 finished with value: 0.08600326665464059 and parameters: {'learning_rate': 0.007821940865964879, 'num_leaves': 471, 'min_data_in_leaf': 1139, 'reg_alpha': 3.529631891337134, 'reg_lambda': 2.3791818081512037, 'colsample_bytree': 0.4234559052081783}. Best is trial 21 with value: 0.07719162594810326.\n",
      "[I 2025-03-29 20:03:43,822] Trial 27 finished with value: 0.08343197476651638 and parameters: {'learning_rate': 0.051672302270648354, 'num_leaves': 512, 'min_data_in_leaf': 909, 'reg_alpha': 1.7367154528645803, 'reg_lambda': 5.479372038503301, 'colsample_bytree': 0.7158996318649856}. Best is trial 21 with value: 0.07719162594810326.\n",
      "[I 2025-03-29 20:40:06,263] Trial 28 finished with value: 0.08115790261168927 and parameters: {'learning_rate': 0.0252586099358009, 'num_leaves': 333, 'min_data_in_leaf': 353, 'reg_alpha': 1.04824934524424, 'reg_lambda': 3.8419776593175308, 'colsample_bytree': 0.4829415327744061}. Best is trial 21 with value: 0.07719162594810326.\n",
      "[I 2025-03-29 20:49:13,471] Trial 29 finished with value: 0.07967247506499195 and parameters: {'learning_rate': 0.15827227147903947, 'num_leaves': 427, 'min_data_in_leaf': 434, 'reg_alpha': 0.12429643336852764, 'reg_lambda': 2.4563608882007193, 'colsample_bytree': 0.31446284755644793}. Best is trial 21 with value: 0.07719162594810326.\n",
      "[I 2025-03-29 21:50:21,767] Trial 30 finished with value: 0.08384275129342887 and parameters: {'learning_rate': 0.008270756441117173, 'num_leaves': 265, 'min_data_in_leaf': 316, 'reg_alpha': 2.105467751959688, 'reg_lambda': 6.9439315937027715, 'colsample_bytree': 0.6335399758655174}. Best is trial 21 with value: 0.07719162594810326.\n",
      "[I 2025-03-30 00:09:33,320] Trial 31 finished with value: 0.07732896566915815 and parameters: {'learning_rate': 0.019995331796609103, 'num_leaves': 511, 'min_data_in_leaf': 109, 'reg_alpha': 0.17847617946330613, 'reg_lambda': 0.9765366602718952, 'colsample_bytree': 0.17853855274481506}. Best is trial 21 with value: 0.07719162594810326.\n",
      "[I 2025-03-30 02:30:42,692] Trial 32 finished with value: 0.07971163317263374 and parameters: {'learning_rate': 0.018483103544969275, 'num_leaves': 474, 'min_data_in_leaf': 157, 'reg_alpha': 0.7334861451306496, 'reg_lambda': 1.083062119389437, 'colsample_bytree': 0.17754317938759848}. Best is trial 21 with value: 0.07719162594810326.\n",
      "[I 2025-03-30 03:18:53,774] Trial 33 finished with value: 0.08273289601682489 and parameters: {'learning_rate': 0.014199545741045845, 'num_leaves': 396, 'min_data_in_leaf': 122, 'reg_alpha': 1.5395001623169267, 'reg_lambda': 1.072012362214108, 'colsample_bytree': 0.5348792136607091}. Best is trial 21 with value: 0.07719162594810326.\n",
      "[I 2025-03-30 03:33:18,145] Trial 34 finished with value: 0.08567973998745548 and parameters: {'learning_rate': 0.028296703690767134, 'num_leaves': 447, 'min_data_in_leaf': 313, 'reg_alpha': 3.443921073302482, 'reg_lambda': 1.9534467265786926, 'colsample_bytree': 0.3971634931579997}. Best is trial 21 with value: 0.07719162594810326.\n",
      "[I 2025-03-30 04:18:21,593] Trial 35 finished with value: 0.07945829353248599 and parameters: {'learning_rate': 0.03618874350318991, 'num_leaves': 484, 'min_data_in_leaf': 675, 'reg_alpha': 0.6774576453739751, 'reg_lambda': 0.6787744605952976, 'colsample_bytree': 0.31057127292391956}. Best is trial 21 with value: 0.07719162594810326.\n",
      "[I 2025-03-30 04:33:51,177] Trial 36 finished with value: 0.08455767259568621 and parameters: {'learning_rate': 0.04538458147605422, 'num_leaves': 511, 'min_data_in_leaf': 416, 'reg_alpha': 2.265989577099511, 'reg_lambda': 3.078354919706173, 'colsample_bytree': 0.17287536814176133}. Best is trial 21 with value: 0.07719162594810326.\n",
      "[I 2025-03-30 04:52:08,175] Trial 37 finished with value: 0.08710977534336904 and parameters: {'learning_rate': 0.020360268013733966, 'num_leaves': 215, 'min_data_in_leaf': 586, 'reg_alpha': 4.803154214533583, 'reg_lambda': 8.253783339625922, 'colsample_bytree': 0.7901765080730017}. Best is trial 21 with value: 0.07719162594810326.\n",
      "[I 2025-03-30 05:15:47,247] Trial 38 finished with value: 0.0790985707272864 and parameters: {'learning_rate': 0.0783222094874403, 'num_leaves': 336, 'min_data_in_leaf': 294, 'reg_alpha': 0.5645206663946503, 'reg_lambda': 1.7933092839077145, 'colsample_bytree': 0.49419707775017424}. Best is trial 21 with value: 0.07719162594810326.\n",
      "[I 2025-03-30 06:19:26,668] Trial 39 finished with value: 0.08252167688317283 and parameters: {'learning_rate': 0.00977686945922679, 'num_leaves': 289, 'min_data_in_leaf': 836, 'reg_alpha': 1.4281830818943848, 'reg_lambda': 4.6191045572303215, 'colsample_bytree': 0.5912867295230952}. Best is trial 21 with value: 0.07719162594810326.\n",
      "[I 2025-03-30 06:46:08,162] Trial 40 finished with value: 0.08549044135249351 and parameters: {'learning_rate': 0.02821295244962942, 'num_leaves': 397, 'min_data_in_leaf': 506, 'reg_alpha': 3.182876961857347, 'reg_lambda': 9.909599046649623, 'colsample_bytree': 0.34934005500760357}. Best is trial 21 with value: 0.07719162594810326.\n",
      "[I 2025-03-30 08:19:58,480] Trial 41 finished with value: 0.077175197831848 and parameters: {'learning_rate': 0.02414570218752482, 'num_leaves': 492, 'min_data_in_leaf': 249, 'reg_alpha': 0.12698809196376012, 'reg_lambda': 0.357320402227454, 'colsample_bytree': 0.2552186891402247}. Best is trial 41 with value: 0.077175197831848.\n",
      "[I 2025-03-30 10:51:12,753] Trial 42 finished with value: 0.0773325656238393 and parameters: {'learning_rate': 0.01373625780425363, 'num_leaves': 484, 'min_data_in_leaf': 237, 'reg_alpha': 0.1883248630516968, 'reg_lambda': 0.8405158799195578, 'colsample_bytree': 0.16734322418035655}. Best is trial 41 with value: 0.077175197831848.\n",
      "[I 2025-03-30 12:05:26,118] Trial 43 finished with value: 0.08094557902401099 and parameters: {'learning_rate': 0.01403693917135243, 'num_leaves': 456, 'min_data_in_leaf': 252, 'reg_alpha': 0.897003081171976, 'reg_lambda': 0.6288244651653487, 'colsample_bytree': 0.15011103920806157}. Best is trial 41 with value: 0.077175197831848.\n",
      "[I 2025-03-30 13:37:32,755] Trial 44 finished with value: 0.0788452356557749 and parameters: {'learning_rate': 0.025195450939880834, 'num_leaves': 481, 'min_data_in_leaf': 219, 'reg_alpha': 0.5369660626424952, 'reg_lambda': 8.980503422777371, 'colsample_bytree': 0.2557523817963527}. Best is trial 41 with value: 0.077175197831848.\n",
      "[I 2025-03-30 13:55:36,968] Trial 45 finished with value: 0.09143493780133902 and parameters: {'learning_rate': 0.0118595442891262, 'num_leaves': 440, 'min_data_in_leaf': 113, 'reg_alpha': 9.963955000835632, 'reg_lambda': 2.1868534660897847, 'colsample_bytree': 0.45915848401464754}. Best is trial 41 with value: 0.077175197831848.\n",
      "[I 2025-03-30 16:22:27,607] Trial 46 finished with value: 0.07827936926833114 and parameters: {'learning_rate': 0.005374118402716297, 'num_leaves': 487, 'min_data_in_leaf': 389, 'reg_alpha': 0.12530039606058024, 'reg_lambda': 0.7076130246560923, 'colsample_bytree': 0.16996526290120265}. Best is trial 41 with value: 0.077175197831848.\n",
      "[I 2025-03-30 16:24:28,086] Trial 47 finished with value: 0.09144088800698932 and parameters: {'learning_rate': 0.18977801685362033, 'num_leaves': 362, 'min_data_in_leaf': 212, 'reg_alpha': 7.462117651337129, 'reg_lambda': 3.0016743182004717, 'colsample_bytree': 0.2863193724896428}. Best is trial 41 with value: 0.077175197831848.\n",
      "[I 2025-03-30 16:47:21,574] Trial 48 finished with value: 0.08337369882488471 and parameters: {'learning_rate': 0.031287681155722565, 'num_leaves': 491, 'min_data_in_leaf': 104, 'reg_alpha': 1.8063884264339205, 'reg_lambda': 1.3121584125492431, 'colsample_bytree': 0.5628513964233735}. Best is trial 41 with value: 0.077175197831848.\n",
      "[I 2025-03-30 17:34:42,619] Trial 49 finished with value: 0.08155911983040429 and parameters: {'learning_rate': 0.018887669934033506, 'num_leaves': 458, 'min_data_in_leaf': 364, 'reg_alpha': 1.117162694316237, 'reg_lambda': 0.5794678478147155, 'colsample_bytree': 0.2274458585381626}. Best is trial 41 with value: 0.077175197831848.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Best hyperparameters found:\n",
      "{'learning_rate': 0.02414570218752482, 'num_leaves': 492, 'min_data_in_leaf': 249, 'reg_alpha': 0.12698809196376012, 'reg_lambda': 0.357320402227454, 'colsample_bytree': 0.2552186891402247}\n",
      "⭐ Best QLIKE: 0.07718\n",
      "\n",
      "🔁 Retraining final model on full data...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "train() got an unexpected keyword argument 'verbose_eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 95\u001b[0m\n\u001b[0;32m     83\u001b[0m best_params\u001b[38;5;241m.\u001b[39mupdate({\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverbosity\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     91\u001b[0m })\n\u001b[0;32m     93\u001b[0m ds_full \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mDataset(X, y, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mpower(y, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m---> 95\u001b[0m final_model \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[0;32m     96\u001b[0m     best_params,\n\u001b[0;32m     97\u001b[0m     ds_full,\n\u001b[0;32m     98\u001b[0m     num_boost_round\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[0;32m     99\u001b[0m     valid_sets\u001b[38;5;241m=\u001b[39m[ds_full],\n\u001b[0;32m    100\u001b[0m     feval\u001b[38;5;241m=\u001b[39mfeval_qlike,\n\u001b[0;32m    101\u001b[0m     verbose_eval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[0;32m    102\u001b[0m )\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# Save final model\u001b[39;00m\n\u001b[0;32m    105\u001b[0m final_model\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlightgbm_qlike_gpu_final_model.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: train() got an unexpected keyword argument 'verbose_eval'"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "# -----------------------------------------------\n",
    "# LIGHTGBM OPTUNA-  COMPLETE\n",
    "# -----------------------------------------------\n",
    "def qlike_from_rv(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # Avoid divide-by-zero or log(0)\n",
    "    eps = 1e-9\n",
    "\n",
    "    exp_y_true = np.exp(y_true) + eps\n",
    "    exp_y_pred = np.exp(y_pred) + eps\n",
    "\n",
    "    qlike = (exp_y_true / exp_y_pred) - (y_true - y_pred) - 1\n",
    "    return np.mean(qlike)\n",
    "\n",
    "\n",
    "def feval_qlike(preds, train_data):\n",
    "    y_true = train_data.get_label()\n",
    "    score = qlike_from_rv(y_true, preds)\n",
    "    return 'QLIKE', abs(score), False  # False = lower is better\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.005, 0.2, log=True)\n",
    "    early_stopping = int(40 * 0.1 / learning_rate)\n",
    "\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'None',  # We'll use feval instead\n",
    "        'verbose': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'device': 'gpu',\n",
    "        'gpu_platform_id': 0,\n",
    "        'gpu_device_id': 0,\n",
    "        'learning_rate': learning_rate,\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 31, 512),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 100, 2000),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 10.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 10.0),\n",
    "        'early_stopping_rounds': early_stopping,\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 1.0),\n",
    "        'max_depth': -1\n",
    "    }\n",
    "\n",
    "    qlike_scores = []\n",
    "\n",
    "    for fold_idx, (train_idx, valid_idx) in enumerate(folds):\n",
    "        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "        X_valid, y_valid = X.iloc[valid_idx], y.iloc[valid_idx]\n",
    "\n",
    "        train_data = lgb.Dataset(X_train, y_train, weight=1 / np.power(y_train, 2))\n",
    "        valid_data = lgb.Dataset(X_valid, y_valid, weight=1 / np.power(y_valid, 2))\n",
    "\n",
    "        booster = lgb.train(\n",
    "            params,\n",
    "            train_data,\n",
    "            valid_sets=[valid_data],\n",
    "            num_boost_round=5000,\n",
    "            feval=feval_qlike,\n",
    "                    )\n",
    "\n",
    "        y_pred = booster.predict(X_valid, num_iteration=booster.best_iteration)\n",
    "        qlike_score = qlike_from_rv(y_valid, y_pred)\n",
    "        qlike_scores.append(qlike_score)\n",
    "\n",
    "    return abs(np.mean(qlike_scores))\n",
    "\n",
    "\n",
    "print(\"\\n🔍 Starting Optuna hyperparameter tuning...\")\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=\"LightGBM_QLIKE_GPU\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "print(\"\\n✅ Best hyperparameters found:\")\n",
    "print(study.best_trial.params)\n",
    "print(f\"⭐ Best QLIKE: {study.best_value:.5f}\")\n",
    "\n",
    "print(\"\\n🔁 Retraining final model on full data...\")\n",
    "\n",
    "best_params = study.best_trial.params\n",
    "best_params.update({\n",
    "    'objective': 'regression',\n",
    "    'metric': 'None',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'device': 'gpu',\n",
    "    'gpu_platform_id': 0,\n",
    "    'gpu_device_id': 0,\n",
    "    'verbosity': -1\n",
    "})\n",
    "\n",
    "ds_full = lgb.Dataset(X, y, weight=1 / np.power(y, 2))\n",
    "\n",
    "final_model = lgb.train(\n",
    "    best_params,\n",
    "    ds_full,\n",
    "    num_boost_round=1000,\n",
    "    valid_sets=[ds_full],\n",
    "    feval=feval_qlike,\n",
    "    \n",
    ")\n",
    "\n",
    "# Save final model\n",
    "final_model.save_model(\"lightgbm_qlike_gpu_final_model.txt\")\n",
    "print(\"\\n💾 Final model saved to 'lightgbm_qlike_gpu_final_model.txt'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a39163-a49d-4432-a955-b2c1ca90c06b",
   "metadata": {},
   "source": [
    "# MLP Optuna Run\n",
    "\n",
    "Applying the same format as used in actual prediction notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6e54847-dbc2-42f9-b001-4674e0236405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import random\n",
    "from typing import List, Tuple, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.decomposition import PCA\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "\n",
    "null_check_cols = [\n",
    "    'book.log_return1.realized_volatility',\n",
    "    'book_150.log_return1.realized_volatility',\n",
    "    'book_300.log_return1.realized_volatility',\n",
    "    'book_450.log_return1.realized_volatility',\n",
    "    'trade.log_return.realized_volatility',\n",
    "    'trade_150.log_return.realized_volatility',\n",
    "    'trade_300.log_return.realized_volatility',\n",
    "    'trade_450.log_return.realized_volatility'\n",
    "]\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def rmspe_metric(y_true, y_pred):\n",
    "    rmspe = np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "    return rmspe\n",
    "\n",
    "\n",
    "def rmspe_loss(y_true, y_pred):\n",
    "    rmspe = torch.sqrt(torch.mean(torch.square((y_true - y_pred) / y_true)))\n",
    "    return rmspe\n",
    "#This is the same RMSPE metric but implemented as a loss function in PyTorch for backpropagation.\n",
    "\n",
    "class RMSPE(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"rmspe\"\n",
    "        self._maximize = False\n",
    "\n",
    "    def __call__(self, y_true, y_score):\n",
    "        return np.sqrt(np.mean(np.square((y_true - y_score) / y_true)))\n",
    "#This class wraps the RMSPE metric into a format that can be used in PyTorch TabNet for evaluation during training.\n",
    "\n",
    "\n",
    "def RMSPELoss_Tabnet(y_pred, y_true):\n",
    "    return torch.sqrt(torch.mean( ((y_true - y_pred) / y_true) ** 2 )).clone()\n",
    "\n",
    "\n",
    "class AverageMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "#used to track running averages of metrics (e.g., loss) during training and evaluation. It's useful for monitoring training progress.\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, x_num: np.ndarray, x_cat: np.ndarray, y: Optional[np.ndarray]):\n",
    "        super().__init__()\n",
    "        self.x_num = x_num\n",
    "        self.x_cat = x_cat\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_num)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.x_num[idx], torch.LongTensor(self.x_cat[idx])\n",
    "        else:\n",
    "            return self.x_num[idx], torch.LongTensor(self.x_cat[idx]), self.y[idx]\n",
    "\n",
    "#splits the data into numeric (x_num) and categorical (x_cat) features and optionally returns the target values (y) for training.\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 src_num_dim: int,\n",
    "                 n_categories: List[int],\n",
    "                 dropout: float = 0.0,\n",
    "                 hidden: int = 50,\n",
    "                 emb_dim: int = 10,\n",
    "                 dropout_cat: float = 0.2,\n",
    "                 bn: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embs = nn.ModuleList([\n",
    "            nn.Embedding(x, emb_dim) for x in n_categories])\n",
    "        self.cat_dim = emb_dim * len(n_categories)\n",
    "        self.dropout_cat = nn.Dropout(dropout_cat)\n",
    "\n",
    "        if bn:\n",
    "            self.sequence = nn.Sequential(\n",
    "                nn.Linear(src_num_dim + self.cat_dim, hidden),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.BatchNorm1d(hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, hidden),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.BatchNorm1d(hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, 1)\n",
    "            )\n",
    "        else:\n",
    "            self.sequence = nn.Sequential(\n",
    "                nn.Linear(src_num_dim + self.cat_dim, hidden),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, hidden),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, 1)\n",
    "            )\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        embs = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embs)]\n",
    "        x_cat_emb = self.dropout_cat(torch.cat(embs, 1))\n",
    "        x_all = torch.cat([x_num, x_cat_emb], 1)\n",
    "        x = self.sequence(x_all)\n",
    "        return torch.squeeze(x)\n",
    "\n",
    "#Embeddings for categorical variables (nn.Embedding).\n",
    "#Dropout to prevent overfitting.\n",
    "#Batch Normalization (optional) for faster convergence and better training stability.\n",
    "#ReLU activation function for non-linearity.\n",
    "#This model is used for regression, where the final output is a single predicted value.\n",
    "\n",
    "    \n",
    "class CNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_features: int,\n",
    "                 hidden_size: int,\n",
    "                 n_categories: List[int],\n",
    "                 emb_dim: int = 10,\n",
    "                 dropout_cat: float = 0.2,\n",
    "                 channel_1: int = 256,\n",
    "                 channel_2: int = 512,\n",
    "                 channel_3: int = 512,\n",
    "                 dropout_top: float = 0.1,\n",
    "                 dropout_mid: float = 0.3,\n",
    "                 dropout_bottom: float = 0.2,\n",
    "                 weight_norm: bool = True,\n",
    "                 two_stage: bool = True,\n",
    "                 celu: bool = True,\n",
    "                 kernel1: int = 5,\n",
    "                 leaky_relu: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        num_targets = 1\n",
    "\n",
    "        cha_1_reshape = int(hidden_size / channel_1)\n",
    "        cha_po_1 = int(hidden_size / channel_1 / 2)\n",
    "        cha_po_2 = int(hidden_size / channel_1 / 2 / 2) * channel_3\n",
    "\n",
    "        self.cat_dim = emb_dim * len(n_categories)\n",
    "        self.cha_1 = channel_1\n",
    "        self.cha_2 = channel_2\n",
    "        self.cha_3 = channel_3\n",
    "        self.cha_1_reshape = cha_1_reshape\n",
    "        self.cha_po_1 = cha_po_1\n",
    "        self.cha_po_2 = cha_po_2\n",
    "        self.two_stage = two_stage\n",
    "\n",
    "        self.expand = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_features + self.cat_dim),\n",
    "            nn.Dropout(dropout_top),\n",
    "            nn.utils.weight_norm(nn.Linear(num_features + self.cat_dim, hidden_size), dim=None),\n",
    "            nn.CELU(0.06) if celu else nn.ReLU()\n",
    "        )\n",
    "\n",
    "        def _norm(layer, dim=None):\n",
    "            return nn.utils.weight_norm(layer, dim=dim) if weight_norm else layer\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.BatchNorm1d(channel_1),\n",
    "            nn.Dropout(dropout_top),\n",
    "            _norm(nn.Conv1d(channel_1, channel_2, kernel_size=kernel1, stride=1, padding=kernel1 // 2, bias=False)),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(output_size=cha_po_1),\n",
    "            nn.BatchNorm1d(channel_2),\n",
    "            nn.Dropout(dropout_top),\n",
    "            _norm(nn.Conv1d(channel_2, channel_2, kernel_size=3, stride=1, padding=1, bias=True)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        if self.two_stage:\n",
    "            self.conv2 = nn.Sequential(\n",
    "                nn.BatchNorm1d(channel_2),\n",
    "                nn.Dropout(dropout_mid),\n",
    "                _norm(nn.Conv1d(channel_2, channel_2, kernel_size=3, stride=1, padding=1, bias=True)),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(channel_2),\n",
    "                nn.Dropout(dropout_bottom),\n",
    "                _norm(nn.Conv1d(channel_2, channel_3, kernel_size=5, stride=1, padding=2, bias=True)),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "\n",
    "        self.max_po_c2 = nn.MaxPool1d(kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        self.flt = nn.Flatten()\n",
    "\n",
    "        if leaky_relu:\n",
    "            self.dense = nn.Sequential(\n",
    "                nn.BatchNorm1d(cha_po_2),\n",
    "                nn.Dropout(dropout_bottom),\n",
    "                _norm(nn.Linear(cha_po_2, num_targets), dim=0),\n",
    "                nn.LeakyReLU()\n",
    "            )\n",
    "        else:\n",
    "            self.dense = nn.Sequential(\n",
    "                nn.BatchNorm1d(cha_po_2),\n",
    "                nn.Dropout(dropout_bottom),\n",
    "                _norm(nn.Linear(cha_po_2, num_targets), dim=0)\n",
    "            )\n",
    "\n",
    "        self.embs = nn.ModuleList([nn.Embedding(x, emb_dim) for x in n_categories])\n",
    "        self.cat_dim = emb_dim * len(n_categories)\n",
    "        self.dropout_cat = nn.Dropout(dropout_cat)\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        embs = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embs)]\n",
    "        x_cat_emb = self.dropout_cat(torch.cat(embs, 1))\n",
    "        x = torch.cat([x_num, x_cat_emb], 1)\n",
    "\n",
    "        x = self.expand(x)\n",
    "\n",
    "        x = x.reshape(x.shape[0], self.cha_1, self.cha_1_reshape)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        if self.two_stage:\n",
    "            x = self.conv2(x) * x\n",
    "\n",
    "        x = self.max_po_c2(x)\n",
    "        x = self.flt(x)\n",
    "        x = self.dense(x)\n",
    "\n",
    "        return torch.squeeze(x)\n",
    "#The CNN class defines a convolutional neural network (CNN) designed for tabular data. Similar to the MLP, it uses:\n",
    "    #Embedding layers for categorical features.\n",
    "    #Convolutional layers for learning spatial hierarchies in data (commonly used for image data but also applied here).\n",
    "    #Pooling layers for downsampling the data.\n",
    "    #This CNN can handle tabular data and learn complex interactions between features.\n",
    "\n",
    "def preprocess_nn(\n",
    "        X: pd.DataFrame,\n",
    "        scaler: Optional[StandardScaler] = None,\n",
    "        scaler_type: str = 'standard',\n",
    "        n_pca: int = -1,\n",
    "        na_cols: bool = True):\n",
    "    if na_cols:\n",
    "        #for c in X.columns:\n",
    "        for c in null_check_cols:\n",
    "            if c in X.columns:\n",
    "                X[f\"{c}_isnull\"] = X[c].isnull().astype(int)\n",
    "\n",
    "    cat_cols = [c for c in X.columns if c in ['time_id', 'stock_id']]\n",
    "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "    X_num = X[num_cols].values.astype(np.float32)\n",
    "    X_cat = np.nan_to_num(X[cat_cols].values.astype(np.int32))\n",
    "\n",
    "    def _pca(X_num_):\n",
    "        if n_pca > 0:\n",
    "            pca = PCA(n_components=n_pca, random_state=0)\n",
    "            return pca.fit_transform(X_num)\n",
    "        return X_num\n",
    "\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        X_num = scaler.fit_transform(X_num)\n",
    "        X_num = np.nan_to_num(X_num, posinf=0, neginf=0)\n",
    "        return _pca(X_num), X_cat, cat_cols, scaler\n",
    "    else:\n",
    "        X_num = scaler.transform(X_num) \n",
    "        X_num = np.nan_to_num(X_num, posinf=0, neginf=0)\n",
    "        return _pca(X_num), X_cat, cat_cols\n",
    "\n",
    "\n",
    "#This function preprocesses the tabular data. It does the following:\n",
    "    #Creates binary columns indicating whether specific features have null values.\n",
    "    #Separates numerical and categorical columns.\n",
    "    #Scales the numerical features using StandardScaler or any other scaling method specified.\n",
    "    #Optionally reduces dimensionality using PCA if n_pca > 0.\n",
    "\n",
    "def train_epoch(data_loader: DataLoader,\n",
    "                model: nn.Module,\n",
    "                optimizer,\n",
    "                scheduler,\n",
    "                device,\n",
    "                clip_grad: float = 1.5):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    step = 0\n",
    "\n",
    "    for x_num, x_cat, y in tqdm(data_loader, position=0, leave=True, desc='Training'):\n",
    "        batch_size = x_num.size(0)\n",
    "        x_num = x_num.to(device, dtype=torch.float)\n",
    "        x_cat = x_cat.to(device)\n",
    "        y = y.to(device, dtype=torch.float)\n",
    "        y_pred = model(x_num, x_cat)\n",
    "        \n",
    "        loss = rmspe_loss(y, model(x_num, x_cat))\n",
    "        losses.update(loss.detach().cpu().numpy(), batch_size)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Scheduler step needs a metric to adjust the learning rate\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(loss)  # Pass validation loss or any relevant metric here\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    return losses.avg\n",
    "#This function trains the model for one epoch. It:\n",
    "    #Loads batches of data.\n",
    "    #Computes the loss using rmspe_loss.\n",
    "    #Performs backpropagation and updates the model weights.\n",
    "    #Optionally applies gradient clipping to prevent exploding gradients.\n",
    "\n",
    "def evaluate(data_loader: DataLoader, model, device):\n",
    "    model.eval()\n",
    "\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    final_targets = []\n",
    "    final_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_num, x_cat, y in tqdm(data_loader, position=0, leave=True, desc='Evaluating'):\n",
    "            batch_size = x_num.size(0)\n",
    "            x_num = x_num.to(device, dtype=torch.float)\n",
    "            x_cat = x_cat.to(device)\n",
    "            y = y.to(device, dtype=torch.float)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(x_num, x_cat)\n",
    "\n",
    "            loss = rmspe_loss(y, output)\n",
    "            # record loss\n",
    "            losses.update(loss.detach().cpu().numpy(), batch_size)\n",
    "\n",
    "            targets = y.detach().cpu().numpy()\n",
    "            output = output.detach().cpu().numpy()\n",
    "\n",
    "            final_targets.append(targets)\n",
    "            final_outputs.append(output)\n",
    "\n",
    "    final_targets = np.concatenate(final_targets)\n",
    "    final_outputs = np.concatenate(final_outputs)\n",
    "\n",
    "    try:\n",
    "        metric = rmspe_metric(final_targets, final_outputs)\n",
    "    except:\n",
    "        metric = None\n",
    "\n",
    "    return final_outputs, final_targets, losses.avg, metric\n",
    "\n",
    "#evaluates the model's performance on the validation set. It computes the RMSPE for the predictions and returns the final predictions, targets and loss.\n",
    "\n",
    "def predict_nn(X: pd.DataFrame,\n",
    "               model: Union[List[MLP], MLP],\n",
    "               scaler: StandardScaler,\n",
    "               device,\n",
    "               ensemble_method='mean'):\n",
    "    if not isinstance(model, list):\n",
    "        model = [model]\n",
    "\n",
    "    for m in model:\n",
    "        m.eval()\n",
    "    X_num, X_cat, cat_cols = preprocess_nn(X.copy(), scaler=scaler)\n",
    "    valid_dataset = TabularDataset(X_num, X_cat, None)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                                               batch_size=512,\n",
    "                                               shuffle=False,\n",
    "                                               num_workers=4)\n",
    "\n",
    "    final_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_num, x_cat in tqdm(valid_loader, position=0, leave=True, desc='Evaluating'):\n",
    "            x_num = x_num.to(device, dtype=torch.float)\n",
    "            x_cat = x_cat.to(device)\n",
    "\n",
    "            outputs = []\n",
    "            with torch.no_grad():\n",
    "                for m in model:\n",
    "                    output = m(x_num, x_cat)\n",
    "                    outputs.append(output.detach().cpu().numpy())\n",
    "\n",
    "            if ensemble_method == 'median':\n",
    "                pred = np.nanmedian(np.array(outputs), axis=0)\n",
    "            else:\n",
    "                pred = np.array(outputs).mean(axis=0)\n",
    "            final_outputs.append(pred)\n",
    "\n",
    "    final_outputs = np.concatenate(final_outputs)\n",
    "    return final_outputs\n",
    "\n",
    "\n",
    "#This function makes predictions using the trained MLP (or an ensemble of MLPs). It takes the input data, preprocesses it, and computes predictions. \n",
    "#The predictions can be averaged or take the median from an ensemble of models.\n",
    "\n",
    "\n",
    "def predict_tabnet(X: pd.DataFrame,\n",
    "                   model: Union[List[TabNetRegressor], TabNetRegressor],\n",
    "                   scaler: StandardScaler,\n",
    "                   ensemble_method='mean'):\n",
    "    if not isinstance(model, list):\n",
    "        model = [model]\n",
    "\n",
    "    X_num, X_cat, cat_cols = preprocess_nn(X.copy(), scaler=scaler)\n",
    "    X_processed = np.concatenate([X_cat, X_num], axis=1)\n",
    "\n",
    "    predicted = []\n",
    "    for m in model:\n",
    "        predicted.append(m.predict(X_processed))\n",
    "\n",
    "    if ensemble_method == 'median':\n",
    "        pred = np.nanmedian(np.array(predicted), axis=0)\n",
    "    else:\n",
    "        pred = np.array(predicted).mean(axis=0)\n",
    "\n",
    "    return pred\n",
    "\n",
    "\n",
    "def train_tabnet(X: pd.DataFrame,\n",
    "                 y: pd.DataFrame,\n",
    "                 folds: List[Tuple],\n",
    "                 batch_size: int = 1024,\n",
    "                 lr: float = 1e-3,\n",
    "                 model_path: str = 'fold_{}.pth',\n",
    "                 scaler_type: str = 'standard',\n",
    "                 output_dir: str = 'artifacts',\n",
    "                 epochs: int = 250,\n",
    "                 seed: int = 42,\n",
    "                 n_pca: int = -1,\n",
    "                 na_cols: bool = True,\n",
    "                 patience: int = 10,\n",
    "                 factor: float = 0.5,\n",
    "                 gamma: float = 2.0,\n",
    "                 lambda_sparse: float = 8.0,\n",
    "                 n_steps: int = 2,\n",
    "                 scheduler_type: str = 'cosine',\n",
    "                 n_a: int = 16):\n",
    "    seed_everything(seed)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    y = y.values.astype(np.float32)\n",
    "    X_num, X_cat, cat_cols, scaler = preprocess_nn(X.copy(), scaler_type=scaler_type, n_pca=n_pca, na_cols=na_cols)\n",
    "\n",
    "    best_losses = []\n",
    "    best_predictions = []\n",
    "\n",
    "    for cv_idx, (train_idx, valid_idx) in enumerate(folds):\n",
    "        X_tr, X_va = X_num[train_idx], X_num[valid_idx]\n",
    "        X_tr_cat, X_va_cat = X_cat[train_idx], X_cat[valid_idx]\n",
    "        y_tr, y_va = y[train_idx], y[valid_idx]\n",
    "        y_tr = y_tr.reshape(-1,1)\n",
    "        y_va = y_va.reshape(-1,1)\n",
    "        X_tr = np.concatenate([X_tr_cat, X_tr], axis=1)\n",
    "        X_va = np.concatenate([X_va_cat, X_va], axis=1)\n",
    "\n",
    "        cat_idxs = [0]\n",
    "        cat_dims = [128]\n",
    "\n",
    "        if scheduler_type == 'cosine':\n",
    "            scheduler_params = dict(T_0=200, T_mult=1, eta_min=1e-4, last_epoch=-1, verbose=False)\n",
    "            scheduler_fn = CosineAnnealingWarmRestarts\n",
    "        else:\n",
    "            scheduler_params = {'mode': 'min', 'min_lr': 1e-7, 'patience': patience, 'factor': factor, 'verbose': True}\n",
    "            scheduler_fn = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "\n",
    "        model = TabNetRegressor(\n",
    "            cat_idxs=cat_idxs,\n",
    "            cat_dims=cat_dims,\n",
    "            cat_emb_dim=1,\n",
    "            n_d=n_a,\n",
    "            n_a=n_a,\n",
    "            n_steps=n_steps,\n",
    "            gamma=gamma,\n",
    "            n_independent=2,\n",
    "            n_shared=2,\n",
    "            lambda_sparse=lambda_sparse,\n",
    "            optimizer_fn=torch.optim.Adam,\n",
    "            optimizer_params={'lr': lr},\n",
    "            mask_type=\"entmax\",\n",
    "            scheduler_fn=scheduler_fn,\n",
    "            scheduler_params=scheduler_params,\n",
    "            seed=seed,\n",
    "            verbose=10\n",
    "            #device_name=device,\n",
    "            #clip_value=1.5\n",
    "        )\n",
    "\n",
    "        model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], max_epochs=epochs, patience=50, batch_size=1024*20,\n",
    "                  virtual_batch_size=batch_size, num_workers=4, drop_last=False, eval_metric=[RMSPE], loss_fn=RMSPELoss_Tabnet)\n",
    "\n",
    "        path = os.path.join(output_dir, model_path.format(cv_idx))\n",
    "        model.save_model(path)\n",
    "\n",
    "        predicted = model.predict(X_va)\n",
    "\n",
    "        rmspe = rmspe_metric(y_va, predicted)\n",
    "        best_losses.append(rmspe)\n",
    "        best_predictions.append(predicted)\n",
    "\n",
    "    return best_losses, best_predictions, scaler, model\n",
    "    \n",
    "#This function trains a TabNet model on the training data. It uses cross-validation and stores the best models and their predictions. \n",
    "#It also applies learning rate schedulers to adjust the learning rate dynamically.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_nn(X: pd.DataFrame,\n",
    "             y: pd.DataFrame,\n",
    "             folds: List[Tuple],\n",
    "             device,\n",
    "             emb_dim: int = 25,\n",
    "             batch_size: int = 1024,  # Lower batch size\n",
    "             model_type: str = 'mlp',\n",
    "             mlp_dropout: float = 0.1,  # Increased dropout\n",
    "             mlp_hidden: int = 128,  # Reduced hidden units\n",
    "             cnn_hidden: int = 128,  # Reduced CNN hidden layers\n",
    "             lr: float = 1e-3,\n",
    "             weight_decay: float = 0.0,\n",
    "             model_path: str = 'fold_{}.pth',\n",
    "             output_dir: str = 'artifacts',\n",
    "             epochs: int = 10,  # Reduced epochs\n",
    "             seed: int = 42,\n",
    "             patience: int = 5,  # Lower patience\n",
    "             factor: float = 0.5):\n",
    "    seed_everything(seed)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    y = y.values.astype(np.float16)  # Lower precision\n",
    "    X_num, X_cat, cat_cols, scaler = preprocess_nn(X.copy(), na_cols=True)\n",
    "    \n",
    "    best_losses = []\n",
    "    best_predictions = []\n",
    "    \n",
    "    for cv_idx, (train_idx, valid_idx) in enumerate(folds):\n",
    "        X_tr, X_va = X_num[train_idx], X_num[valid_idx]\n",
    "        y_tr, y_va = y[train_idx], y[valid_idx]\n",
    "        \n",
    "        train_dataset = TabularDataset(X_tr, X_cat[train_idx] if X_cat is not None else None, y_tr)\n",
    "        valid_dataset = TabularDataset(X_va, X_cat[valid_idx] if X_cat is not None else None, y_va)\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "        valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "        # Handle n_categories correctly for MLP\n",
    "        n_categories = [X_cat[:, i].max() + 1 for i in range(X_cat.shape[1])] if X_cat is not None else []\n",
    "        \n",
    "        model = MLP(X_tr.shape[1], n_categories=n_categories, hidden=mlp_hidden, dropout=mlp_dropout, emb_dim=emb_dim).to(device)\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', patience=patience, factor=factor)\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        best_prediction = None\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            train_loss = train_epoch(train_loader, model, opt, scheduler, device)\n",
    "            with torch.no_grad():  # Disable gradients in evaluation\n",
    "                predictions, valid_targets, valid_loss, rmspe = evaluate(valid_loader, model, device=device)\n",
    "            \n",
    "            if rmspe < best_loss:\n",
    "                best_loss = rmspe\n",
    "                best_prediction = predictions\n",
    "                torch.save(model, os.path.join(output_dir, model_path.format(cv_idx)))\n",
    "\n",
    "        best_predictions.append(best_prediction)\n",
    "        best_losses.append(best_loss)\n",
    "        \n",
    "        del model, train_loader, valid_loader, train_dataset, valid_dataset\n",
    "        gc.collect()\n",
    "\n",
    "    return best_losses, best_predictions, scaler\n",
    "\n",
    "\n",
    "#This function trains a neural network (either MLP or CNN) on the training data. It involves:\n",
    "    #Initializing the model.\n",
    "    #Using an optimizer (Adam, AdamW).\n",
    "    #Training for multiple epochs and evaluating the performance using RMSPE.\n",
    "    #Optionally using learning rate schedulers (like CosineAnnealingWarmRestarts) for training dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f8c9ccd-8ba0-41d6-aed3-143cf60a1d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_models(models, scores, top_n):\n",
    "    if len(models) <= top_n:\n",
    "        print('Number of models are less than top_n. All models will be used.')\n",
    "        return models\n",
    "    sorted_ = [(y, x) for y, x in sorted(zip(scores, models), key=lambda pair: pair[0])]\n",
    "    print(f'Scores (sorted): {[y for y, _ in sorted_]}')\n",
    "    return [x for _, x in sorted_][:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3cb6367-7b33-49e7-9765-4645f0c9238b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-01 22:26:07,117] A new study created in memory with name: no-name-5efe0de0-87a4-4672-9af8-84d42c34bf3f\n",
      "[I 2025-04-01 22:28:50,383] Trial 0 finished with value: 0.05634798854589462 and parameters: {'hidden_dim': 78, 'dropout_rate': 0.36664184682667167, 'lr': 0.00011773611629994392, 'batch_size': 32}. Best is trial 0 with value: 0.05634798854589462.\n",
      "[I 2025-04-01 22:31:58,305] Trial 1 finished with value: 0.05276545509696007 and parameters: {'hidden_dim': 205, 'dropout_rate': 0.1124315267701379, 'lr': 0.00015805704074040478, 'batch_size': 32}. Best is trial 1 with value: 0.05276545509696007.\n",
      "[I 2025-04-01 22:35:52,950] Trial 2 finished with value: 0.2025771290063858 and parameters: {'hidden_dim': 232, 'dropout_rate': 0.4956638204164292, 'lr': 0.002118689676858749, 'batch_size': 32}. Best is trial 1 with value: 0.05276545509696007.\n",
      "[I 2025-04-01 22:38:10,484] Trial 3 finished with value: 0.09652992337942123 and parameters: {'hidden_dim': 202, 'dropout_rate': 0.24896090210486177, 'lr': 0.001788514339433075, 'batch_size': 64}. Best is trial 1 with value: 0.05276545509696007.\n",
      "[I 2025-04-01 22:39:37,338] Trial 4 finished with value: 0.06519322842359543 and parameters: {'hidden_dim': 104, 'dropout_rate': 0.21646056443356812, 'lr': 0.009046486950347276, 'batch_size': 128}. Best is trial 1 with value: 0.05276545509696007.\n",
      "[I 2025-04-01 22:42:42,564] Trial 5 finished with value: 0.09953776746988297 and parameters: {'hidden_dim': 102, 'dropout_rate': 0.12661516351735017, 'lr': 0.00538355961499985, 'batch_size': 32}. Best is trial 1 with value: 0.05276545509696007.\n",
      "[I 2025-04-01 22:45:07,886] Trial 6 finished with value: 0.06557713449001312 and parameters: {'hidden_dim': 225, 'dropout_rate': 0.39288274305681115, 'lr': 0.0027977335552504747, 'batch_size': 64}. Best is trial 1 with value: 0.05276545509696007.\n",
      "[I 2025-04-01 22:46:52,301] Trial 7 finished with value: 0.05786841735243797 and parameters: {'hidden_dim': 68, 'dropout_rate': 0.37596438938790566, 'lr': 0.00015800421951052163, 'batch_size': 64}. Best is trial 1 with value: 0.05276545509696007.\n",
      "[I 2025-04-01 22:49:00,466] Trial 8 finished with value: 0.05288592353463173 and parameters: {'hidden_dim': 213, 'dropout_rate': 0.27617902455381893, 'lr': 0.0006089759777922057, 'batch_size': 64}. Best is trial 1 with value: 0.05276545509696007.\n",
      "[I 2025-04-01 22:50:44,530] Trial 9 finished with value: 0.057876940816640854 and parameters: {'hidden_dim': 80, 'dropout_rate': 0.3670875175174422, 'lr': 0.00012754773988046038, 'batch_size': 64}. Best is trial 1 with value: 0.05276545509696007.\n",
      "[I 2025-04-01 22:52:15,956] Trial 10 finished with value: 0.05039172247052193 and parameters: {'hidden_dim': 170, 'dropout_rate': 0.10368936480428612, 'lr': 0.00045183922948921024, 'batch_size': 128}. Best is trial 10 with value: 0.05039172247052193.\n",
      "[I 2025-04-01 22:53:45,902] Trial 11 finished with value: 0.050834160298109055 and parameters: {'hidden_dim': 164, 'dropout_rate': 0.11252354974119777, 'lr': 0.0003855747426372951, 'batch_size': 128}. Best is trial 10 with value: 0.05039172247052193.\n",
      "[I 2025-04-01 22:55:15,934] Trial 12 finished with value: 0.05688716843724251 and parameters: {'hidden_dim': 163, 'dropout_rate': 0.17791744416073818, 'lr': 0.0005217818499849979, 'batch_size': 128}. Best is trial 10 with value: 0.05039172247052193.\n",
      "[I 2025-04-01 22:56:45,755] Trial 13 finished with value: 0.05847148597240448 and parameters: {'hidden_dim': 162, 'dropout_rate': 0.17672209794894317, 'lr': 0.00038197510599693237, 'batch_size': 128}. Best is trial 10 with value: 0.05039172247052193.\n",
      "[I 2025-04-01 22:58:10,881] Trial 14 finished with value: 0.05100902169942856 and parameters: {'hidden_dim': 140, 'dropout_rate': 0.10004480984299947, 'lr': 0.0002974830791536698, 'batch_size': 128}. Best is trial 10 with value: 0.05039172247052193.\n",
      "[I 2025-04-01 22:59:45,351] Trial 15 finished with value: 0.05628843605518341 and parameters: {'hidden_dim': 177, 'dropout_rate': 0.15552720309003673, 'lr': 0.0009293773504427962, 'batch_size': 128}. Best is trial 10 with value: 0.05039172247052193.\n",
      "[I 2025-04-01 23:01:29,612] Trial 16 finished with value: 0.05454031378030777 and parameters: {'hidden_dim': 254, 'dropout_rate': 0.2206243300728704, 'lr': 0.00028015590319565496, 'batch_size': 128}. Best is trial 10 with value: 0.05039172247052193.\n",
      "[I 2025-04-01 23:02:54,203] Trial 17 finished with value: 0.06228949874639511 and parameters: {'hidden_dim': 132, 'dropout_rate': 0.3013819279350074, 'lr': 0.0009490021383874872, 'batch_size': 128}. Best is trial 10 with value: 0.05039172247052193.\n",
      "[I 2025-04-01 23:04:27,856] Trial 18 finished with value: 0.05112019181251526 and parameters: {'hidden_dim': 184, 'dropout_rate': 0.14257724803445695, 'lr': 0.00023333271459474012, 'batch_size': 128}. Best is trial 10 with value: 0.05039172247052193.\n",
      "[I 2025-04-01 23:05:52,365] Trial 19 finished with value: 0.06446579843759537 and parameters: {'hidden_dim': 140, 'dropout_rate': 0.20041309939161928, 'lr': 0.0006060591611559931, 'batch_size': 128}. Best is trial 10 with value: 0.05039172247052193.\n",
      "[I 2025-04-01 23:07:27,591] Trial 20 finished with value: 0.07513986527919769 and parameters: {'hidden_dim': 185, 'dropout_rate': 0.3332165134543894, 'lr': 0.0013076690011713241, 'batch_size': 128}. Best is trial 10 with value: 0.05039172247052193.\n",
      "[I 2025-04-01 23:08:53,706] Trial 21 finished with value: 0.05339778587222099 and parameters: {'hidden_dim': 137, 'dropout_rate': 0.10010756423776587, 'lr': 0.0002917078329719118, 'batch_size': 128}. Best is trial 10 with value: 0.05039172247052193.\n",
      "[I 2025-04-01 23:10:15,883] Trial 22 finished with value: 0.053047653287649155 and parameters: {'hidden_dim': 122, 'dropout_rate': 0.10058835153619806, 'lr': 0.00043191631732637933, 'batch_size': 128}. Best is trial 10 with value: 0.05039172247052193.\n",
      "[I 2025-04-01 23:11:43,639] Trial 23 finished with value: 0.05336795374751091 and parameters: {'hidden_dim': 152, 'dropout_rate': 0.15194977860155282, 'lr': 0.00022771648505890283, 'batch_size': 128}. Best is trial 10 with value: 0.05039172247052193.\n",
      "[I 2025-04-01 23:13:16,832] Trial 24 finished with value: 0.06289678066968918 and parameters: {'hidden_dim': 174, 'dropout_rate': 0.17365887768361438, 'lr': 0.0007367823204207577, 'batch_size': 128}. Best is trial 10 with value: 0.05039172247052193.\n",
      "[I 2025-04-01 23:14:38,088] Trial 25 finished with value: 0.05402194336056709 and parameters: {'hidden_dim': 114, 'dropout_rate': 0.13455063312372464, 'lr': 0.0003219611925794859, 'batch_size': 128}. Best is trial 10 with value: 0.05039172247052193.\n",
      "[I 2025-04-01 23:16:06,243] Trial 26 finished with value: 0.06719086319208145 and parameters: {'hidden_dim': 153, 'dropout_rate': 0.4389351417627437, 'lr': 0.0004369213179094655, 'batch_size': 128}. Best is trial 10 with value: 0.05039172247052193.\n",
      "[I 2025-04-01 23:17:39,389] Trial 27 finished with value: 0.061913974583148956 and parameters: {'hidden_dim': 194, 'dropout_rate': 0.23840022818660603, 'lr': 0.00019525969610962023, 'batch_size': 128}. Best is trial 10 with value: 0.05039172247052193.\n",
      "[I 2025-04-01 23:19:08,850] Trial 28 finished with value: 0.06261204183101654 and parameters: {'hidden_dim': 149, 'dropout_rate': 0.19323590611123403, 'lr': 0.0012284535481632073, 'batch_size': 128}. Best is trial 10 with value: 0.05039172247052193.\n",
      "[I 2025-04-01 23:21:59,354] Trial 29 finished with value: 0.05251629650592804 and parameters: {'hidden_dim': 168, 'dropout_rate': 0.12877946415402994, 'lr': 0.00011554906059831272, 'batch_size': 32}. Best is trial 10 with value: 0.05039172247052193.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters from Optuna: {'hidden_dim': 170, 'dropout_rate': 0.10368936480428612, 'lr': 0.00045183922948921024, 'batch_size': 128}\n",
      "Training model 1/10\n",
      "Saved model 1 to mlp_ensemble_model_seed0.pt\n",
      "Training model 2/10\n",
      "Saved model 2 to mlp_ensemble_model_seed1.pt\n",
      "Training model 3/10\n",
      "Saved model 3 to mlp_ensemble_model_seed2.pt\n",
      "Training model 4/10\n",
      "Saved model 4 to mlp_ensemble_model_seed3.pt\n",
      "Training model 5/10\n",
      "Saved model 5 to mlp_ensemble_model_seed4.pt\n",
      "Training model 6/10\n",
      "Saved model 6 to mlp_ensemble_model_seed5.pt\n",
      "Training model 7/10\n",
      "Saved model 7 to mlp_ensemble_model_seed6.pt\n",
      "Training model 8/10\n",
      "Saved model 8 to mlp_ensemble_model_seed7.pt\n",
      "Training model 9/10\n",
      "Saved model 9 to mlp_ensemble_model_seed8.pt\n",
      "Training model 10/10\n",
      "Saved model 10 to mlp_ensemble_model_seed9.pt\n",
      "All ensemble models trained and saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import optuna\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ----------------------------\n",
    "# QLIKE METRIC\n",
    "# ----------------------------\n",
    "def qlike_from_rv(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    eps = 1e-9\n",
    "    exp_y_true = np.exp(y_true) + eps\n",
    "    exp_y_pred = np.exp(y_pred) + eps\n",
    "    qlike = (exp_y_true / exp_y_pred) - (y_true - y_pred) - 1\n",
    "    return np.mean(qlike)\n",
    "\n",
    "# ----------------------------\n",
    "# MLP Model Definition\n",
    "# ----------------------------\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, dropout_rate):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze()\n",
    "\n",
    "# ----------------------------\n",
    "# Dataset Wrapper\n",
    "# ----------------------------\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# ----------------------------\n",
    "# Prepare Data\n",
    "# ----------------------------\n",
    "scaler_X = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "X_scaled = np.array(X_scaled, dtype=np.float32)\n",
    "y_scaled = np.array(y_scaled, dtype=np.float32)\n",
    "\n",
    "# ----------------------------\n",
    "# Optuna Hyperparameter Search\n",
    "# ----------------------------\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int('hidden_dim', 64, 256)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "\n",
    "    model = MLP(X_scaled.shape[1], hidden_dim, dropout_rate).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    dataset = TabularDataset(torch.tensor(X_scaled), torch.tensor(y_scaled))\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(10):\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model(torch.tensor(X_scaled).to(device)).cpu().numpy()\n",
    "    return qlike_from_rv(y_scaled, preds)\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(\"Best Parameters from Optuna:\", best_params)\n",
    "\n",
    "# ----------------------------\n",
    "# Train Ensemble of MLP Models\n",
    "# ----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_models = 10\n",
    "ensemble_models = []\n",
    "\n",
    "for seed in range(n_models):\n",
    "    print(f\"Training model {seed+1}/{n_models}\")\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    model = MLP(X_scaled.shape[1], best_params[\"hidden_dim\"], best_params[\"dropout_rate\"]).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=best_params[\"lr\"])\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    train_loader = DataLoader(TabularDataset(torch.tensor(X_scaled), torch.tensor(y_scaled)),\n",
    "                              batch_size=best_params[\"batch_size\"], shuffle=True)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(50):\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model_path = f\"mlp_ensemble_model_seed{seed}.pt\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    ensemble_models.append(model_path)\n",
    "    print(f\"Saved model {seed+1} to {model_path}\")\n",
    "\n",
    "print(\"All ensemble models trained and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2513bd02-ba9c-47cf-b7a0-d60a5fcfc629",
   "metadata": {},
   "source": [
    "# LSTM Optuna Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a02863d1-199a-43b7-b0b9-0b1acfbb1fe7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-30 20:26:58,095] A new study created in memory with name: no-name-d80447f4-e110-4bdd-b0c9-83e8074b78f5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 995us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-30 20:50:08,030] Trial 0 finished with value: 0.0508580282330513 and parameters: {'lstm_units_1': 116, 'lstm_units_2': 21, 'dropout_rate': 0.1735857065047698, 'learning_rate': 0.00026493359085830637, 'batch_size': 64}. Best is trial 0 with value: 0.0508580282330513.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-30 21:06:11,605] Trial 1 finished with value: 0.0527987964451313 and parameters: {'lstm_units_1': 114, 'lstm_units_2': 37, 'dropout_rate': 0.39883280196430393, 'learning_rate': 0.0008593721322829184, 'batch_size': 128}. Best is trial 0 with value: 0.0508580282330513.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 986us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 980us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 992us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-30 21:37:42,716] Trial 2 finished with value: 0.06253718584775925 and parameters: {'lstm_units_1': 72, 'lstm_units_2': 54, 'dropout_rate': 0.4017988770435714, 'learning_rate': 0.0037526567714114105, 'batch_size': 32}. Best is trial 0 with value: 0.0508580282330513.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 941us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 922us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 972us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 916us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-30 21:47:12,267] Trial 3 finished with value: 0.05378023907542229 and parameters: {'lstm_units_1': 58, 'lstm_units_2': 36, 'dropout_rate': 0.3596619776122173, 'learning_rate': 0.0003223752539092304, 'batch_size': 128}. Best is trial 0 with value: 0.0508580282330513.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 964us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 967us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 960us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 946us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-30 22:25:19,749] Trial 4 finished with value: 0.05303065478801727 and parameters: {'lstm_units_1': 110, 'lstm_units_2': 25, 'dropout_rate': 0.38366104446507043, 'learning_rate': 0.0006051654681774414, 'batch_size': 32}. Best is trial 0 with value: 0.0508580282330513.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 993us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 979us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step  \n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 996us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-30 23:10:43,537] Trial 5 finished with value: 0.050663575530052185 and parameters: {'lstm_units_1': 109, 'lstm_units_2': 47, 'dropout_rate': 0.11768495972706439, 'learning_rate': 0.00017818644728731514, 'batch_size': 32}. Best is trial 5 with value: 0.050663575530052185.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 967us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 955us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 966us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 983us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-30 23:41:55,780] Trial 6 finished with value: 0.052009325474500656 and parameters: {'lstm_units_1': 98, 'lstm_units_2': 16, 'dropout_rate': 0.4180573050430745, 'learning_rate': 0.00027965666854605416, 'batch_size': 32}. Best is trial 5 with value: 0.050663575530052185.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 872us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 886us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 887us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 906us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-30 23:50:31,156] Trial 7 finished with value: 0.0501602441072464 and parameters: {'lstm_units_1': 40, 'lstm_units_2': 51, 'dropout_rate': 0.16302829280601264, 'learning_rate': 0.00011268118341176139, 'batch_size': 128}. Best is trial 7 with value: 0.0501602441072464.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 00:03:10,126] Trial 8 finished with value: 0.052093956619501114 and parameters: {'lstm_units_1': 101, 'lstm_units_2': 40, 'dropout_rate': 0.38429715066057113, 'learning_rate': 0.0008516513649835571, 'batch_size': 128}. Best is trial 7 with value: 0.0501602441072464.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 970us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 921us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 896us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 927us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 00:13:40,608] Trial 9 finished with value: 0.07908257842063904 and parameters: {'lstm_units_1': 65, 'lstm_units_2': 29, 'dropout_rate': 0.48824532162745615, 'learning_rate': 0.0006582552448147639, 'batch_size': 128}. Best is trial 7 with value: 0.0501602441072464.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 925us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 893us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 880us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 930us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 00:27:45,516] Trial 10 finished with value: 0.05267965793609619 and parameters: {'lstm_units_1': 39, 'lstm_units_2': 63, 'dropout_rate': 0.2381433835710563, 'learning_rate': 0.00010865769026922112, 'batch_size': 64}. Best is trial 7 with value: 0.0501602441072464.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 955us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 899us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 895us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 879us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 00:51:46,960] Trial 11 finished with value: 0.050772227346897125 and parameters: {'lstm_units_1': 34, 'lstm_units_2': 47, 'dropout_rate': 0.10756636734831823, 'learning_rate': 0.00011205252406316834, 'batch_size': 32}. Best is trial 7 with value: 0.0501602441072464.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 963us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 982us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 982us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 982us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 01:04:08,171] Trial 12 finished with value: 0.056907590478658676 and parameters: {'lstm_units_1': 86, 'lstm_units_2': 50, 'dropout_rate': 0.1018344628826725, 'learning_rate': 0.003323565591945337, 'batch_size': 128}. Best is trial 7 with value: 0.0501602441072464.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 913us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 922us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 917us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 905us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 01:30:38,427] Trial 13 finished with value: 0.07079530507326126 and parameters: {'lstm_units_1': 52, 'lstm_units_2': 60, 'dropout_rate': 0.20762949592295615, 'learning_rate': 0.007833733863957564, 'batch_size': 32}. Best is trial 7 with value: 0.0501602441072464.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 02:03:05,976] Trial 14 finished with value: 0.05071312189102173 and parameters: {'lstm_units_1': 128, 'lstm_units_2': 47, 'dropout_rate': 0.27407816667767815, 'learning_rate': 0.00017487160575908555, 'batch_size': 64}. Best is trial 7 with value: 0.0501602441072464.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 985us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 949us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 976us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 968us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 02:15:16,396] Trial 15 finished with value: 0.05388972908258438 and parameters: {'lstm_units_1': 79, 'lstm_units_2': 55, 'dropout_rate': 0.1589835134747519, 'learning_rate': 0.0016021672069807538, 'batch_size': 128}. Best is trial 7 with value: 0.0501602441072464.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 956us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 972us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 984us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 02:46:45,159] Trial 16 finished with value: 0.05037800222635269 and parameters: {'lstm_units_1': 92, 'lstm_units_2': 46, 'dropout_rate': 0.1477277387199562, 'learning_rate': 0.00017091486267337708, 'batch_size': 32}. Best is trial 7 with value: 0.0501602441072464.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 978us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 961us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 986us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 967us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 02:58:14,740] Trial 17 finished with value: 0.051070116460323334 and parameters: {'lstm_units_1': 90, 'lstm_units_2': 31, 'dropout_rate': 0.3086800988363981, 'learning_rate': 0.00043974485431527434, 'batch_size': 128}. Best is trial 7 with value: 0.0501602441072464.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 923us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 892us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 901us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 910us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 03:24:29,546] Trial 18 finished with value: 0.05146744102239609 and parameters: {'lstm_units_1': 46, 'lstm_units_2': 42, 'dropout_rate': 0.17866962794910687, 'learning_rate': 0.00018335780159762106, 'batch_size': 32}. Best is trial 7 with value: 0.0501602441072464.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 977us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 972us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 987us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 03:41:58,458] Trial 19 finished with value: 0.058032866567373276 and parameters: {'lstm_units_1': 73, 'lstm_units_2': 57, 'dropout_rate': 0.24225491748882563, 'learning_rate': 0.001606747151615237, 'batch_size': 64}. Best is trial 7 with value: 0.0501602441072464.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 933us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 927us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 939us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 03:52:03,538] Trial 20 finished with value: 0.05021350085735321 and parameters: {'lstm_units_1': 60, 'lstm_units_2': 52, 'dropout_rate': 0.3116454554244704, 'learning_rate': 0.00011088432241813655, 'batch_size': 128}. Best is trial 7 with value: 0.0501602441072464.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 941us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 931us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 959us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 04:02:22,075] Trial 21 finished with value: 0.05034686625003815 and parameters: {'lstm_units_1': 59, 'lstm_units_2': 51, 'dropout_rate': 0.30896637428726176, 'learning_rate': 0.00011523237820084088, 'batch_size': 128}. Best is trial 7 with value: 0.0501602441072464.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 923us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 940us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 936us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 955us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 04:12:38,003] Trial 22 finished with value: 0.05134715512394905 and parameters: {'lstm_units_1': 60, 'lstm_units_2': 53, 'dropout_rate': 0.32229190649468686, 'learning_rate': 0.00010226068131120033, 'batch_size': 128}. Best is trial 7 with value: 0.0501602441072464.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 927us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 939us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 946us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 940us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 04:22:19,085] Trial 23 finished with value: 0.05175969749689102 and parameters: {'lstm_units_1': 49, 'lstm_units_2': 60, 'dropout_rate': 0.3384919504864672, 'learning_rate': 0.00013960304310834325, 'batch_size': 128}. Best is trial 7 with value: 0.0501602441072464.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 908us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 947us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 927us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 934us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 04:31:13,720] Trial 24 finished with value: 0.057025182992219925 and parameters: {'lstm_units_1': 41, 'lstm_units_2': 51, 'dropout_rate': 0.29163778705873633, 'learning_rate': 0.00025903255900694677, 'batch_size': 128}. Best is trial 7 with value: 0.0501602441072464.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 945us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 955us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 948us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 961us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 04:41:42,636] Trial 25 finished with value: 0.051118403673172 and parameters: {'lstm_units_1': 64, 'lstm_units_2': 42, 'dropout_rate': 0.2642447514876116, 'learning_rate': 0.0003539457503847089, 'batch_size': 128}. Best is trial 7 with value: 0.0501602441072464.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 968us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 934us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 944us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 937us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 04:51:57,069] Trial 26 finished with value: 0.05328996479511261 and parameters: {'lstm_units_1': 55, 'lstm_units_2': 59, 'dropout_rate': 0.2120786473584095, 'learning_rate': 0.000472032576462358, 'batch_size': 128}. Best is trial 7 with value: 0.0501602441072464.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 902us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 901us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 882us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 897us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 05:00:02,984] Trial 27 finished with value: 0.08142340928316116 and parameters: {'lstm_units_1': 32, 'lstm_units_2': 63, 'dropout_rate': 0.35020199088534704, 'learning_rate': 0.00021904738962278444, 'batch_size': 128}. Best is trial 7 with value: 0.0501602441072464.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 942us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 921us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 942us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 05:09:22,070] Trial 28 finished with value: 0.06322826445102692 and parameters: {'lstm_units_1': 43, 'lstm_units_2': 51, 'dropout_rate': 0.45855888601964784, 'learning_rate': 0.00014043955562628588, 'batch_size': 128}. Best is trial 7 with value: 0.0501602441072464.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 947us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 944us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 953us/step\n",
      "\u001b[1m1341/1341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 952us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 05:26:07,779] Trial 29 finished with value: 0.050079721957445145 and parameters: {'lstm_units_1': 69, 'lstm_units_2': 44, 'dropout_rate': 0.3020399589467795, 'learning_rate': 0.00013315950716143908, 'batch_size': 64}. Best is trial 29 with value: 0.050079721957445145.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'lstm_units_1': 69, 'lstm_units_2': 44, 'dropout_rate': 0.3020399589467795, 'learning_rate': 0.00013315950716143908, 'batch_size': 64}\n",
      "Epoch 1/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - loss: 0.1625\n",
      "Epoch 2/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.1096\n",
      "Epoch 3/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.1065\n",
      "Epoch 4/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.1051\n",
      "Epoch 5/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.1034\n",
      "Epoch 6/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.1031\n",
      "Epoch 7/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.1019\n",
      "Epoch 8/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.1020\n",
      "Epoch 9/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.1007\n",
      "Epoch 10/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.0997\n",
      "Epoch 11/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.1005\n",
      "Epoch 12/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.0997\n",
      "Epoch 13/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.0988\n",
      "Epoch 14/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.0996\n",
      "Epoch 15/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.0992\n",
      "Epoch 16/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.0980\n",
      "Epoch 17/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.0973\n",
      "Epoch 18/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.0974\n",
      "Epoch 19/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.0971\n",
      "Epoch 20/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.0967\n",
      "Epoch 21/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.0957\n",
      "Epoch 22/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.0955\n",
      "Epoch 23/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.0958\n",
      "Epoch 24/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.0957\n",
      "Epoch 25/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.0952\n",
      "Epoch 26/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.0948\n",
      "Epoch 27/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.0941\n",
      "Epoch 28/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 0.0938\n",
      "Epoch 29/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.0941\n",
      "Epoch 30/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.0939\n",
      "Epoch 31/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.0934\n",
      "Epoch 32/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.0932\n",
      "Epoch 33/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 0.0931\n",
      "Epoch 34/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.0924\n",
      "Epoch 35/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.0925\n",
      "Epoch 36/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.0921\n",
      "Epoch 37/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.0928\n",
      "Epoch 38/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.0915\n",
      "Epoch 39/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.0918\n",
      "Epoch 40/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.0916\n",
      "Epoch 41/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.0908\n",
      "Epoch 42/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.0910\n",
      "Epoch 43/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.0912\n",
      "Epoch 44/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.0907\n",
      "Epoch 45/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.0908\n",
      "Epoch 46/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.0899\n",
      "Epoch 47/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.0907\n",
      "Epoch 48/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.0906\n",
      "Epoch 49/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.0903\n",
      "Epoch 50/50\n",
      "\u001b[1m6703/6703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.0899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Training Completed and Model Saved.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import optuna\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# QLIKE metric\n",
    "def qlike_from_rv(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    eps = 1e-9\n",
    "    exp_y_true = np.exp(y_true) + eps\n",
    "    exp_y_pred = np.exp(y_pred) + eps\n",
    "    qlike = (exp_y_true / exp_y_pred) - (y_true - y_pred) - 1\n",
    "    return np.mean(qlike)\n",
    "\n",
    "# Prepare data\n",
    "X_np = np.array(X, dtype=np.float32)\n",
    "if np.isnan(X_np).any():\n",
    "    X = X.fillna(method='ffill')\n",
    "if np.isnan(y).any():\n",
    "    print(\"Warning: NaN values detected in y!\")\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X_np)\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "X_np = np.array(X_scaled, dtype=np.float32)\n",
    "y_np = np.array(y_scaled, dtype=np.float32)\n",
    "X_np = X_np.reshape((X_np.shape[0], 1, X_np.shape[1]))\n",
    "\n",
    "# Optuna objective using QLIKE\n",
    "def objective(trial):\n",
    "    lstm_units_1 = trial.suggest_int(\"lstm_units_1\", 32, 128)\n",
    "    lstm_units_2 = trial.suggest_int(\"lstm_units_2\", 16, 64)\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "    epochs = 20\n",
    "\n",
    "    fold_scores = []\n",
    "\n",
    "    for train_idx, valid_idx in folds:\n",
    "        X_train, X_valid = X_np[train_idx], X_np[valid_idx]\n",
    "        y_train, y_valid = y_np[train_idx], y_np[valid_idx]\n",
    "\n",
    "        model = Sequential([\n",
    "            tf.keras.layers.Input(shape=(1, X_train.shape[2])),\n",
    "            LSTM(lstm_units_1, return_sequences=True),\n",
    "            Dropout(dropout_rate),\n",
    "            LSTM(lstm_units_2, return_sequences=False),\n",
    "            Dense(16, activation=\"relu\"),\n",
    "            Dense(1, activation=\"linear\")\n",
    "        ])\n",
    "\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss=\"mse\")\n",
    "\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_valid, y_valid),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        y_pred = model.predict(X_valid).flatten()\n",
    "        score = qlike_from_rv(y_valid, y_pred)\n",
    "        fold_scores.append(score)\n",
    "\n",
    "    return np.mean(fold_scores)\n",
    "\n",
    "# Run Optuna tuning\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "# Best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Train final model on full data\n",
    "final_model = Sequential([\n",
    "    tf.keras.layers.Input(shape=(1, X_np.shape[2])),\n",
    "    LSTM(best_params[\"lstm_units_1\"], return_sequences=True),\n",
    "    Dropout(best_params[\"dropout_rate\"]),\n",
    "    LSTM(best_params[\"lstm_units_2\"], return_sequences=False),\n",
    "    Dense(16, activation=\"relu\"),\n",
    "    Dense(1, activation=\"linear\")\n",
    "])\n",
    "\n",
    "final_optimizer = Adam(learning_rate=best_params[\"learning_rate\"])\n",
    "final_model.compile(optimizer=final_optimizer, loss=\"mse\")\n",
    "final_model.fit(X_np, y_np, epochs=50, batch_size=best_params[\"batch_size\"], verbose=1)\n",
    "\n",
    "# Save model\n",
    "final_model.save(\"lstm_optuna_model.h5\")\n",
    "print(\"LSTM Training Completed and Model Saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
